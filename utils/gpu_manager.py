# utils/gpu_manager.py - è‡ªåŠ¨æ˜¾å­˜ç®¡ç† + æ™ºèƒ½æ¨¡å‹é€‰æ‹©
"""
SmartVideoClipper - GPUæ˜¾å­˜ç®¡ç†æ¨¡å—

åŠŸèƒ½:
1. è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°
2. æ ¹æ®æ˜¾å­˜é€‰æ‹©æœ€ä¼˜æ¨¡å‹é…ç½®
3. æ¯æ­¥åè‡ªåŠ¨æ¸…ç†æ˜¾å­˜

æ”¯æŒ: GTX 1080åŠä»¥ä¸Šæ‰€æœ‰NVIDIAæ˜¾å¡
"""

import torch
import gc


class GPUManager:
    """è‡ªåŠ¨ç®¡ç†GPUæ˜¾å­˜ï¼Œæ”¯æŒGTX 1080åŠä»¥ä¸Šæ‰€æœ‰æ˜¾å¡"""
    
    # ä¸åŒæ˜¾å­˜å¯¹åº”çš„æ¨¡å‹é…ç½®
    MODEL_CONFIGS = {
        6: {  # 6GBæ˜¾å­˜ (GTX 1060, RTX 2060)
            'whisper': 'small',
            'clip': 'ViT-B-16',
            'qwen': 'qwen2.5:3b'
        },
        8: {  # 8GBæ˜¾å­˜ (GTX 1080, RTX 3060) [STAR]æ¨è
            'whisper': 'medium',
            'clip': 'ViT-B-16',
            'qwen': 'qwen2.5:7b'
        },
        12: {  # 12GBæ˜¾å­˜ (RTX 3060Ti, RTX 4070)
            'whisper': 'large-v2',
            'clip': 'ViT-L-14',
            'qwen': 'qwen2.5:14b'
        },
        16: {  # 16GB+æ˜¾å­˜ (RTX 4080, RTX 4090)
            'whisper': 'large-v3',
            'clip': 'ViT-H-14',
            'qwen': 'qwen2.5:32b'
        }
    }
    
    @staticmethod
    def clear():
        """æ¸…ç†GPUæ˜¾å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    
    @staticmethod
    def get_total_memory():
        """è·å–æ€»æ˜¾å­˜(GB)"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
        return 0
    
    @staticmethod
    def get_free_memory():
        """è·å–å‰©ä½™æ˜¾å­˜(MB)"""
        if torch.cuda.is_available():
            free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
            return free / 1024 / 1024
        return 0
    
    @classmethod
    def get_optimal_config(cls):
        """
        ğŸ”¥ è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°ï¼Œè¿”å›æœ€ä¼˜æ¨¡å‹é…ç½®
        æ”¯æŒGTX 1080åŠä»¥ä¸Šæ‰€æœ‰NVIDIAæ˜¾å¡
        """
        total_gb = cls.get_total_memory()
        
        # é€‰æ‹©åˆé€‚çš„é…ç½®æ¡£ä½
        if total_gb >= 16:
            config_key = 16
        elif total_gb >= 12:
            config_key = 12
        elif total_gb >= 8:
            config_key = 8
        else:
            config_key = 6
        
        config = cls.MODEL_CONFIGS[config_key]
        print(f"[GPU] æ£€æµ‹åˆ°æ˜¾å­˜: {total_gb:.1f}GB")
        print(f"[LIST] è‡ªåŠ¨é€‰æ‹©é…ç½®: Whisper={config['whisper']}, CLIP={config['clip']}, Qwen={config['qwen']}")
        
        return config
    
    @staticmethod
    def is_cuda_available():
        """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
        return torch.cuda.is_available()
    
    @staticmethod
    def get_device():
        """è·å–è®¾å¤‡ï¼ˆcudaæˆ–cpuï¼‰"""
        return "cuda" if torch.cuda.is_available() else "cpu"
    
    @staticmethod
    def get_device_name():
        """è·å–GPUåç§°"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_name(0)
        return "CPU"


# æ¨¡å—çº§åˆ«å˜é‡ï¼Œæ–¹ä¾¿å…¶ä»–æ¨¡å—ä½¿ç”¨
config = None


def init_config():
    """åˆå§‹åŒ–é…ç½®ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ï¼‰"""
    global config
    config = GPUManager.get_optimal_config()
    return config


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("=" * 50)
    print("GPU Manager - æ˜¾å­˜æ£€æµ‹")
    print("=" * 50)
    
    # æ£€æµ‹GPU
    print(f"\nCUDAå¯ç”¨: {GPUManager.is_cuda_available()}")
    print(f"è®¾å¤‡: {GPUManager.get_device()}")
    print(f"GPUåç§°: {GPUManager.get_device_name()}")
    print(f"æ€»æ˜¾å­˜: {GPUManager.get_total_memory():.1f}GB")
    print(f"å‰©ä½™æ˜¾å­˜: {GPUManager.get_free_memory():.0f}MB")
    
    # è·å–æœ€ä¼˜é…ç½®
    print("\nè·å–æœ€ä¼˜é…ç½®:")
    config = GPUManager.get_optimal_config()
    
    print("\næµ‹è¯•æ˜¾å­˜æ¸…ç†:")
    GPUManager.clear()
    print("[OK] æ˜¾å­˜å·²æ¸…ç†")
