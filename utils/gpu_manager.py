# utils/gpu_manager.py - è‡ªåŠ¨æ˜¾å­˜ç®¡ç† + OllamaæœåŠ¡ç›‘æ§
"""
SmartVideoClipper - GPUæ˜¾å­˜ç®¡ç†æ¨¡å— v5.9

åŠŸèƒ½:
1. è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°å’Œä½¿ç”¨ç‡
2. æ ¹æ®æ˜¾å­˜é€‰æ‹©æœ€ä¼˜æ¨¡å‹é…ç½®
3. å®æ—¶ç›‘æ§æ˜¾å­˜å ç”¨ï¼Œè‡ªåŠ¨æ¸…ç†
4. OllamaæœåŠ¡é‡å¯å’ŒçŠ¶æ€ç›‘æ§
5. æ™ºèƒ½é™çº§ç­–ç•¥ï¼ˆGPU -> CPUï¼‰

æ”¯æŒ: GTX 1080åŠä»¥ä¸Šæ‰€æœ‰NVIDIAæ˜¾å¡
RTX 4060ä¼˜åŒ–: 8GBæ˜¾å­˜ä¸“ç”¨é…ç½®
"""

import torch
import gc
import psutil
import subprocess
import time
import os
from typing import Optional, Dict, Any


class GPUManager:
    """è‡ªåŠ¨ç®¡ç†GPUæ˜¾å­˜ + OllamaæœåŠ¡ï¼Œæ”¯æŒGTX 1080åŠä»¥ä¸Šæ‰€æœ‰æ˜¾å¡"""
    
    # RTX 4060ä¸“ç”¨é…ç½® (8GBæ˜¾å­˜ä¼˜åŒ–)
    MODEL_CONFIGS = {
        6: {  # 6GBæ˜¾å­˜ (GTX 1060, RTX 2060)
            'whisper': 'small',
            'clip': 'ViT-B-16',
            'qwen': 'qwen2.5:3b'
        },
        8: {  # 8GBæ˜¾å­˜ - RTX 4060ä¸“ç”¨ä¼˜åŒ–
            'whisper': 'medium',
            'clip': 'ViT-B-16',
            'qwen': 'qwen2.5:7b',  # é™çº§ä»¥èŠ‚çœæ˜¾å­˜
            # åˆ†çº§æ¨¡å‹é…ç½® (RTX 4060ä¼˜åŒ–)
            'story_framework': 'gemma3:4b',    # 3.3GB - å‰æœŸå¤„ç†
            'hook_generator': 'gemma3:4b',     # 3.3GB - é’©å­ç”Ÿæˆ
            'silence_handler': 'codellama',    # 3.8GB - é™éŸ³å¤„ç†
            'narration': 'qwen3:8b',          # 5.2GB - æ ¸å¿ƒè§£è¯´(æœ€åä½¿ç”¨)
        },
        12: {  # 12GBæ˜¾å­˜ (RTX 3060Ti, RTX 4070)
            'whisper': 'large-v2',
            'clip': 'ViT-L-14',
            'qwen': 'qwen2.5:14b'
        },
        16: {  # 16GB+æ˜¾å­˜ (RTX 4080, RTX 4090)
            'whisper': 'large-v3',
            'clip': 'ViT-H-14',
            'qwen': 'qwen2.5:32b'
        }
    }
    
    @staticmethod
    def clear():
        """æ¸…ç†GPUæ˜¾å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    
    @staticmethod
    def get_total_memory():
        """è·å–æ€»æ˜¾å­˜(GB)"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024
        return 0
    
    @staticmethod
    def get_free_memory():
        """è·å–å‰©ä½™æ˜¾å­˜(MB)"""
        if torch.cuda.is_available():
            free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
            return free / 1024 / 1024
        return 0

    @staticmethod
    def get_memory_usage():
        """è·å–æ˜¾å­˜ä½¿ç”¨ç‡ (0.0-1.0)"""
        if torch.cuda.is_available():
            total = torch.cuda.get_device_properties(0).total_memory
            used = torch.cuda.memory_allocated(0)
            return used / total
        return 0.0

    @staticmethod
    def get_memory_info():
        """è·å–è¯¦ç»†æ˜¾å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            total = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024  # GB
            allocated = torch.cuda.memory_allocated(0) / 1024 / 1024 / 1024  # GB
            reserved = torch.cuda.memory_reserved(0) / 1024 / 1024 / 1024   # GB
            free = total - allocated

            return {
                'total_gb': total,
                'allocated_gb': allocated,
                'reserved_gb': reserved,
                'free_gb': free,
                'usage_percent': (allocated / total) * 100
            }
        return None

    @staticmethod
    def check_memory_threshold(threshold: float = 0.85) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡æ˜¾å­˜é˜ˆå€¼"""
        usage = GPUManager.get_memory_usage()
        return usage > threshold

    @staticmethod
    def cleanup_memory():
        """å¼ºåˆ¶æ¸…ç†GPUæ˜¾å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ç­‰å¾…æ¸…ç†å®Œæˆ
            gc.collect()
            print(f"[GPU] æ˜¾å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨ç‡: {GPUManager.get_memory_usage():.1%}")

    @staticmethod
    def is_ollama_running() -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ"""
        try:
            result = subprocess.run(['tasklist', '/FI', 'IMAGENAME eq ollama.exe'],
                                  capture_output=True, text=True, shell=True)
            return 'ollama.exe' in result.stdout
        except:
            return False

    @staticmethod
    def restart_ollama_service():
        """é‡å¯OllamaæœåŠ¡ä»¥æ¸…ç†æ˜¾å­˜"""
        print("[Ollama] é‡å¯æœåŠ¡ä»¥æ¸…ç†æ˜¾å­˜...")

        try:
            # åœæ­¢Ollama
            subprocess.run(['taskkill', '/F', '/IM', 'ollama.exe'], shell=True,
                         capture_output=True)
            subprocess.run(['taskkill', '/F', '/IM', 'ollama app.exe'], shell=True,
                         capture_output=True)

            time.sleep(2)  # ç­‰å¾…åœæ­¢å®Œæˆ

            # å¯åŠ¨Ollama
            subprocess.Popen(['ollama', 'serve'], shell=True,
                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            time.sleep(3)

            if GPUManager.is_ollama_running():
                print("[Ollama] æœåŠ¡é‡å¯æˆåŠŸ")
                return True
            else:
                print("[Ollama] æœåŠ¡é‡å¯å¤±è´¥")
                return False

        except Exception as e:
            print(f"[Ollama] é‡å¯å¤±è´¥: {e}")
            return False

    @staticmethod
    def monitor_and_cleanup(threshold: float = 0.85):
        """ç›‘æ§æ˜¾å­˜å¹¶è‡ªåŠ¨æ¸…ç†"""
        if GPUManager.check_memory_threshold(threshold):
            usage = GPUManager.get_memory_usage()
            print(".1%")
            print("[GPU] è§¦å‘è‡ªåŠ¨æ¸…ç†...")

            # å…ˆå°è¯•æ¸…ç†GPUæ˜¾å­˜
            GPUManager.cleanup_memory()

            # å¦‚æœæ¸…ç†åä»ç„¶è¶…è¿‡é˜ˆå€¼ï¼Œé‡å¯Ollama
            if GPUManager.check_memory_threshold(threshold):
                print("[GPU] GPUæ¸…ç†æ— æ•ˆï¼Œé‡å¯OllamaæœåŠ¡...")
                if GPUManager.restart_ollama_service():
                    print("[GPU] Ollamaé‡å¯åæ˜¾å­˜çŠ¶æ€æ­£å¸¸")
                else:
                    print("[GPU] Ollamaé‡å¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨å¹²é¢„")
                    return False

            return True
        return True
    
    @classmethod
    def get_optimal_config(cls):
        """
        ğŸ”¥ è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°ï¼Œè¿”å›æœ€ä¼˜æ¨¡å‹é…ç½®
        æ”¯æŒGTX 1080åŠä»¥ä¸Šæ‰€æœ‰NVIDIAæ˜¾å¡
        RTX 4060ä¸“ç”¨: åˆ†çº§æ¨¡å‹ç­–ç•¥
        """
        total_gb = cls.get_total_memory()

        # é€‰æ‹©åˆé€‚çš„é…ç½®æ¡£ä½ï¼ˆä½¿ç”¨0.5GBå®¹å·®ï¼Œé¿å…7.99GBè¢«åˆ¤ä¸º<8GBï¼‰
        if total_gb >= 15.5:
            config_key = 16
        elif total_gb >= 11.5:
            config_key = 12
        elif total_gb >= 7.5:  # RTX 4060 8GB å®é™…æ˜¾ç¤º7.99GB
            config_key = 8
        else:
            config_key = 6

        config = cls.MODEL_CONFIGS[config_key]
        print(f"[GPU] æ£€æµ‹åˆ°æ˜¾å­˜: {total_gb:.1f}GB (æ¡£ä½: {config_key}GB)")

        if config_key == 8:
            print("[RTX4060] å¯ç”¨åˆ†çº§æ¨¡å‹ç­–ç•¥:")
            print(f"  â”œâ”€â”€ æ•…äº‹æ¡†æ¶: {config['story_framework']}")
            print(f"  â”œâ”€â”€ é’©å­ç”Ÿæˆ: {config['hook_generator']}")
            print(f"  â”œâ”€â”€ é™éŸ³å¤„ç†: {config['silence_handler']}")
            print(f"  â””â”€â”€ æ ¸å¿ƒè§£è¯´: {config['narration']}")
        else:
            print(f"[LIST] æ ‡å‡†é…ç½®: Whisper={config['whisper']}, CLIP={config['clip']}, Qwen={config['qwen']}")

        return config

    @classmethod
    def get_model_for_task(cls, task: str) -> str:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹è¿”å›æœ€ä¼˜æ¨¡å‹ (RTX 4060ä¼˜åŒ–)
        """
        config = cls.get_optimal_config()

        # RTX 4060åˆ†çº§ç­–ç•¥
        if cls.get_total_memory() >= 7.5:  # RTX 4060
            task_models = {
                'story_framework': config.get('story_framework', config['qwen']),
                'hook_generator': config.get('hook_generator', config['qwen']),
                'silence_handler': config.get('silence_handler', config['qwen']),
                'narration': config.get('narration', config['qwen']),
                'default': config['qwen']
            }
            return task_models.get(task, task_models['default'])

        # å…¶ä»–æ˜¾å­˜é…ç½®ä½¿ç”¨é»˜è®¤æ¨¡å‹
        return config['qwen']
    
    @staticmethod
    def is_cuda_available():
        """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨"""
        return torch.cuda.is_available()
    
    @staticmethod
    def get_device():
        """è·å–è®¾å¤‡ï¼ˆcudaæˆ–cpuï¼‰"""
        return "cuda" if torch.cuda.is_available() else "cpu"
    
    @staticmethod
    def get_device_name():
        """è·å–GPUåç§°"""
        if torch.cuda.is_available():
            return torch.cuda.get_device_name(0)
        return "CPU"


# æ¨¡å—çº§åˆ«å˜é‡ï¼Œæ–¹ä¾¿å…¶ä»–æ¨¡å—ä½¿ç”¨
config = None


def init_config():
    """åˆå§‹åŒ–é…ç½®ï¼ˆå¯åŠ¨æ—¶è°ƒç”¨ä¸€æ¬¡ï¼‰"""
    global config
    config = GPUManager.get_optimal_config()
    return config


# RTX 4060æ™ºèƒ½æ˜¾å­˜ç®¡ç†æµ‹è¯•
if __name__ == "__main__":
    print("=" * 60)
    print("GPU Manager v5.9 - RTX 4060æ™ºèƒ½æ˜¾å­˜ç®¡ç†")
    print("=" * 60)

    # æ£€æµ‹GPU
    print(f"\nğŸ” ç³»ç»Ÿæ£€æµ‹:")
    print(f"   CUDAå¯ç”¨: {GPUManager.is_cuda_available()}")
    print(f"   è®¾å¤‡: {GPUManager.get_device()}")
    print(f"   GPUåç§°: {GPUManager.get_device_name()}")
    print(f"   Ollamaè¿è¡Œ: {GPUManager.is_ollama_running()}")

    if GPUManager.is_cuda_available():
        mem_info = GPUManager.get_memory_info()
        print(f"   æ€»æ˜¾å­˜: {mem_info['total_gb']:.1f}GB")
        print(f"   å·²åˆ†é…: {mem_info['allocated_gb']:.1f}GB")
        print(f"   å·²ä¿ç•™: {mem_info['reserved_gb']:.1f}GB")
        print(f"   å‰©ä½™: {mem_info['free_gb']:.1f}GB")
        print(f"   ä½¿ç”¨ç‡: {mem_info['usage_percent']:.1f}%")

    # è·å–æœ€ä¼˜é…ç½®
    print(f"\nğŸ¯ æ™ºèƒ½é…ç½®:")
    config = GPUManager.get_optimal_config()

    # RTX 4060åˆ†çº§æ¨¡å‹æ¼”ç¤º
    if GPUManager.get_total_memory() >= 7.5:
        print(f"\nğŸ”§ RTX 4060åˆ†çº§ç­–ç•¥æµ‹è¯•:")
        tasks = ['story_framework', 'hook_generator', 'silence_handler', 'narration']
        for task in tasks:
            model = GPUManager.get_model_for_task(task)
            print(f"   {task}: {model}")

    # æ˜¾å­˜ç›‘æ§æµ‹è¯•
    print(f"\nğŸ“Š æ˜¾å­˜ç›‘æ§æµ‹è¯•:")
    usage = GPUManager.get_memory_usage()
    print(f"   å½“å‰ä½¿ç”¨ç‡: {usage:.1%}")

    threshold = 0.85
    if GPUManager.check_memory_threshold(threshold):
        print(f"   âš ï¸ è¶…è¿‡{threshold:.0%}é˜ˆå€¼ï¼Œå‡†å¤‡æ¸…ç†...")
        GPUManager.monitor_and_cleanup(threshold)
    else:
        print(f"   âœ… æ˜¾å­˜ä½¿ç”¨æ­£å¸¸ (é˜ˆå€¼: {threshold:.0%})")

    print(f"\nğŸ§¹ æ¸…ç†æµ‹è¯•:")
    GPUManager.cleanup_memory()
    print("[OK] æ˜¾å­˜ç®¡ç†æµ‹è¯•å®Œæˆ")
