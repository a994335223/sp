# core/video_content_analyzer.py - è§†é¢‘å†…å®¹åˆ†æå™¨
"""
SmartVideoClipper v4.0 - åŸºäºè§†é¢‘å†…å®¹çš„è§£è¯´ç”Ÿæˆ

æ ¸å¿ƒç†å¿µï¼šçœ‹ç”»é¢å†™è§£è¯´ï¼Œä¸æ˜¯å†™è§£è¯´æ‰¾ç”»é¢

å·¥ä½œæµç¨‹ï¼š
1. åˆ†æè§†é¢‘æ¯ä¸ªåœºæ™¯çš„å†…å®¹ï¼ˆç”¨CLIPåˆ†æç”»é¢ï¼‰
2. ç»“åˆå­—å¹•ç†è§£æ¯ä¸ªåœºæ™¯åœ¨è®²ä»€ä¹ˆ
3. ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆå¯¹åº”çš„è§£è¯´
4. è§£è¯´å’Œç”»é¢ä¸€ä¸€å¯¹åº”

è¿™æ‰æ˜¯çœŸæ­£çš„è§£è¯´åšä¸»åšæ³•ï¼
"""

import cv2
import numpy as np
import torch
from typing import Dict, List, Tuple, Optional
import os
from pathlib import Path


class VideoContentAnalyzer:
    """
    è§†é¢‘å†…å®¹åˆ†æå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼šåˆ†æè§†é¢‘æ¯ä¸ªåœºæ™¯çš„å†…å®¹ï¼Œç”Ÿæˆç»“æ„åŒ–æè¿°
    """
    
    def __init__(self):
        self.clip_model = None
        self.preprocess = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self._load_models()
    
    def _load_models(self):
        """åŠ è½½CLIPæ¨¡å‹"""
        try:
            import cn_clip.clip as clip
            from cn_clip.clip import load_from_name
            
            print("[CLIP] åŠ è½½è§†è§‰åˆ†ææ¨¡å‹...")
            self.clip_model, self.preprocess = load_from_name(
                "ViT-B-16",
                device=self.device,
                download_root='./models'
            )
            self.clip_model.eval()
            self.tokenizer = clip.tokenize
            print(f"[CLIP] æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
        except Exception as e:
            print(f"[WARNING] CLIPåŠ è½½å¤±è´¥: {e}")
            self.clip_model = None
    
    def analyze_video(
        self,
        video_path: str,
        scenes: List[Dict],
        transcript_segments: List[Dict],
        sample_interval: float = 5.0
    ) -> List[Dict]:
        """
        åˆ†æè§†é¢‘å†…å®¹
        
        å‚æ•°ï¼š
            video_path: è§†é¢‘è·¯å¾„
            scenes: åœºæ™¯åˆ—è¡¨ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            transcript_segments: å­—å¹•ç‰‡æ®µ
            sample_interval: é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
        
        è¿”å›ï¼š
        [
            {
                'scene_id': 1,
                'start_time': 0.0,
                'end_time': 30.0,
                'visual_content': 'ä¸€ä¸ªç”·äººåœ¨è¡—ä¸Šèµ°',
                'dialogue': 'ä½ å¥½å•Š',
                'emotion': 'å¹³é™',
                'scene_type': 'dialogue',  # dialogue/action/transition/emotion
                'importance': 0.8,
                'suggested_narration': 'ç”»é¢ä¸­ï¼Œç”·ä¸»è§’...',
                'keep_original_audio': False,
            },
            ...
        ]
        """
        print("\n" + "="*60)
        print("ğŸ¬ è§†é¢‘å†…å®¹åˆ†æå™¨ v4.0")
        print("   æ ¸å¿ƒï¼šçœ‹ç”»é¢å†™è§£è¯´")
        print("="*60)
        
        if not os.path.exists(video_path):
            print(f"[ERROR] è§†é¢‘ä¸å­˜åœ¨: {video_path}")
            return []
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps if fps > 0 else 0
        
        print(f"   è§†é¢‘æ—¶é•¿: {total_duration:.0f}ç§’")
        print(f"   åœºæ™¯æ•°é‡: {len(scenes)}")
        print(f"   å­—å¹•æ•°é‡: {len(transcript_segments)}")
        
        # åˆ†ææ¯ä¸ªåœºæ™¯
        analyzed_scenes = []
        
        # åœºæ™¯åˆ†ç±»æ ‡ç­¾ï¼ˆç”¨äºCLIPåˆ†æï¼‰
        scene_labels = [
            "ä¸¤ä¸ªäººåœ¨å¯¹è¯", "ä¸€ä¸ªäººåœ¨è¯´è¯", "æ‰“æ–—åœºé¢", "è¿½é€åœºé¢",
            "å®‰é™çš„åœºæ™¯", "ç´§å¼ çš„åœºé¢", "æ‚²ä¼¤çš„åœºæ™¯", "å¿«ä¹çš„åœºæ™¯",
            "å®¤å†…åœºæ™¯", "å®¤å¤–åœºæ™¯", "å¤œæ™šåœºæ™¯", "ç™½å¤©åœºæ™¯",
            "ç‰¹å†™é•œå¤´", "è¿œæ™¯é•œå¤´", "ä¼šè®®åœºæ™¯", "åƒé¥­åœºæ™¯",
            "å¼€è½¦åœºæ™¯", "èµ°è·¯åœºæ™¯", "æ‹¥æŠ±åœºæ™¯", "å“­æ³£åœºæ™¯"
        ]
        
        # æƒ…æ„Ÿè¯æ±‡
        emotion_keywords = {
            'tense': ['å¿«', 'å°å¿ƒ', 'å±é™©', 'è·‘', 'è¿½', 'æ€', 'æª'],
            'sad': ['å“­', 'å¯¹ä¸èµ·', 'æ­»', 'å¤±å»', 'å†è§', 'ç¦»å¼€'],
            'angry': ['æ»š', 'æ··è›‹', 'ä¸ºä»€ä¹ˆ', 'å‡­ä»€ä¹ˆ'],
            'happy': ['å“ˆå“ˆ', 'å¤ªå¥½äº†', 'å¼€å¿ƒ', 'å–œæ¬¢'],
            'neutral': []
        }
        
        print("\n[åˆ†æ] å¼€å§‹é€åœºæ™¯åˆ†æ...")
        
        for i, scene in enumerate(scenes[:50]):  # é™åˆ¶åˆ†æå‰50ä¸ªåœºæ™¯
            scene_start = scene.get('start', scene.get('start_time', 0))
            scene_end = scene.get('end', scene.get('end_time', scene_start + 5))
            
            # è½¬æ¢å¸§æ•°åˆ°ç§’æ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if scene_start > 1000:
                scene_start = scene_start / fps
                scene_end = scene_end / fps
            
            # åˆ†æè§†è§‰å†…å®¹
            visual_content = ""
            scene_type = "unknown"
            
            if self.clip_model:
                mid_time = (scene_start + scene_end) / 2
                cap.set(cv2.CAP_PROP_POS_MSEC, mid_time * 1000)
                ret, frame = cap.read()
                
                if ret and frame is not None:
                    visual_content, scene_type = self._analyze_frame_content(
                        frame, scene_labels
                    )
            
            # è·å–è¯¥åœºæ™¯çš„å¯¹è¯
            scene_dialogues = []
            for seg in transcript_segments:
                seg_start = seg.get('start', 0)
                seg_end = seg.get('end', seg_start + 3)
                if seg_start >= scene_start and seg_end <= scene_end + 5:
                    scene_dialogues.append(seg.get('text', ''))
            
            dialogue_text = ' '.join(scene_dialogues)
            
            # åˆ¤æ–­æƒ…æ„Ÿ
            emotion = self._detect_emotion(dialogue_text, emotion_keywords)
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™åŸå£°
            keep_original = self._should_keep_original(
                dialogue_text, visual_content, scene_type, emotion
            )
            
            # è®¡ç®—é‡è¦æ€§
            importance = self._calculate_importance(
                dialogue_text, visual_content, scene_type, emotion
            )
            
            analyzed_scene = {
                'scene_id': i + 1,
                'start_time': scene_start,
                'end_time': scene_end,
                'duration': scene_end - scene_start,
                'visual_content': visual_content,
                'dialogue': dialogue_text[:200] if dialogue_text else '',
                'emotion': emotion,
                'scene_type': scene_type,
                'importance': importance,
                'keep_original_audio': keep_original,
            }
            
            analyzed_scenes.append(analyzed_scene)
            
            if (i + 1) % 10 == 0:
                print(f"   å·²åˆ†æ {i+1}/{min(len(scenes), 50)} ä¸ªåœºæ™¯")
        
        cap.release()
        
        # æŒ‰é‡è¦æ€§æ’åºï¼Œé€‰æ‹©å…³é”®åœºæ™¯
        analyzed_scenes.sort(key=lambda x: x['importance'], reverse=True)
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼Œå…± {len(analyzed_scenes)} ä¸ªåœºæ™¯")
        print(f"   é«˜é‡è¦æ€§åœºæ™¯: {sum(1 for s in analyzed_scenes if s['importance'] > 0.7)}")
        print(f"   ä¿ç•™åŸå£°åœºæ™¯: {sum(1 for s in analyzed_scenes if s['keep_original_audio'])}")
        
        return analyzed_scenes
    
    def _analyze_frame_content(
        self,
        frame: np.ndarray,
        labels: List[str]
    ) -> Tuple[str, str]:
        """ç”¨CLIPåˆ†æå¸§å†…å®¹"""
        try:
            from PIL import Image
            import cn_clip.clip as clip
            
            # è½¬æ¢å›¾åƒ
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # é¢„å¤„ç†
            image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)
            text_inputs = self.tokenizer(labels).to(self.device)
            
            with torch.no_grad():
                image_features = self.clip_model.encode_image(image_input)
                text_features = self.clip_model.encode_text(text_inputs)
                
                image_features /= image_features.norm(dim=-1, keepdim=True)
                text_features /= text_features.norm(dim=-1, keepdim=True)
                
                similarity = (image_features @ text_features.T).squeeze()
                
                # è·å–topåŒ¹é…
                top_idx = similarity.argmax().item()
                top_label = labels[top_idx]
                
                # åˆ¤æ–­åœºæ™¯ç±»å‹
                if any(kw in top_label for kw in ['å¯¹è¯', 'è¯´è¯']):
                    scene_type = 'dialogue'
                elif any(kw in top_label for kw in ['æ‰“æ–—', 'è¿½é€', 'ç´§å¼ ']):
                    scene_type = 'action'
                elif any(kw in top_label for kw in ['æ‚²ä¼¤', 'å“­æ³£']):
                    scene_type = 'emotion'
                else:
                    scene_type = 'transition'
                
                return top_label, scene_type
                
        except Exception as e:
            return "æœªçŸ¥åœºæ™¯", "unknown"
    
    def _detect_emotion(self, dialogue: str, emotion_keywords: Dict) -> str:
        """æ£€æµ‹æƒ…æ„Ÿ"""
        for emotion, keywords in emotion_keywords.items():
            if any(kw in dialogue for kw in keywords):
                return emotion
        return 'neutral'
    
    def _should_keep_original(
        self,
        dialogue: str,
        visual: str,
        scene_type: str,
        emotion: str
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿ç•™åŸå£°"""
        # é‡è¦å¯¹è¯åœºæ™¯ä¿ç•™åŸå£°
        if scene_type == 'dialogue' and len(dialogue) > 30:
            return True
        
        # é«˜æƒ…æ„Ÿå¼ºåº¦åœºæ™¯ä¿ç•™åŸå£°
        if emotion in ['tense', 'sad', 'angry']:
            return True
        
        # åŠ¨ä½œåœºæ™¯ä¿ç•™åŸå£°
        if scene_type == 'action':
            return True
        
        return False
    
    def _calculate_importance(
        self,
        dialogue: str,
        visual: str,
        scene_type: str,
        emotion: str
    ) -> float:
        """è®¡ç®—åœºæ™¯é‡è¦æ€§"""
        score = 0.5  # åŸºç¡€åˆ†
        
        # æœ‰å¯¹è¯åŠ åˆ†
        if dialogue:
            score += 0.1 + min(0.2, len(dialogue) / 200)
        
        # æƒ…æ„Ÿåœºæ™¯åŠ åˆ†
        if emotion in ['tense', 'sad', 'angry']:
            score += 0.2
        
        # åŠ¨ä½œåœºæ™¯åŠ åˆ†
        if scene_type == 'action':
            score += 0.15
        
        # å¯¹è¯åœºæ™¯åŠ åˆ†
        if scene_type == 'dialogue':
            score += 0.1
        
        return min(1.0, score)
    
    def generate_scene_narrations(
        self,
        analyzed_scenes: List[Dict],
        target_duration: int,
        style: str = "å¹½é»˜"
    ) -> List[Dict]:
        """
        ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆå¯¹åº”çš„è§£è¯´
        
        è¿™æ˜¯å…³é”®ï¼è§£è¯´æ˜¯é’ˆå¯¹å…·ä½“ç”»é¢çš„ï¼Œä¸æ˜¯æ³›æ³›è€Œè°ˆ
        """
        print("\n" + "="*60)
        print("ğŸ“ åœºæ™¯è§£è¯´ç”Ÿæˆå™¨")
        print(f"   ç›®æ ‡æ—¶é•¿: {target_duration}ç§’")
        print("="*60)
        
        # æŒ‰æ—¶é—´æ’åº
        scenes = sorted(analyzed_scenes, key=lambda x: x['start_time'])
        
        # è®¡ç®—éœ€è¦å¤šå°‘åœºæ™¯
        avg_scene_duration = 15  # å¹³å‡æ¯ä¸ªåœºæ™¯15ç§’
        needed_scenes = target_duration // avg_scene_duration
        
        # é€‰æ‹©æœ€é‡è¦çš„åœºæ™¯
        important_scenes = sorted(scenes, key=lambda x: x['importance'], reverse=True)
        selected_scenes = important_scenes[:needed_scenes]
        selected_scenes.sort(key=lambda x: x['start_time'])  # æŒ‰æ—¶é—´æ’åº
        
        print(f"   é€‰æ‹©äº† {len(selected_scenes)} ä¸ªå…³é”®åœºæ™¯")
        
        # ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆè§£è¯´
        result_scenes = []
        
        for scene in selected_scenes:
            if scene['keep_original_audio']:
                # ä¿ç•™åŸå£°çš„åœºæ™¯ï¼Œä¸éœ€è¦è§£è¯´
                scene['narration'] = ''
                scene['narration_type'] = 'original'
            else:
                # éœ€è¦è§£è¯´çš„åœºæ™¯
                narration = self._generate_single_narration(
                    scene, style
                )
                scene['narration'] = narration
                scene['narration_type'] = 'voiceover'
            
            result_scenes.append(scene)
        
        print(f"   è§£è¯´åœºæ™¯: {sum(1 for s in result_scenes if s['narration_type'] == 'voiceover')}")
        print(f"   åŸå£°åœºæ™¯: {sum(1 for s in result_scenes if s['narration_type'] == 'original')}")
        
        return result_scenes
    
    def _generate_single_narration(self, scene: Dict, style: str) -> str:
        """ä¸ºå•ä¸ªåœºæ™¯ç”Ÿæˆè§£è¯´"""
        visual = scene.get('visual_content', '')
        dialogue = scene.get('dialogue', '')
        emotion = scene.get('emotion', 'neutral')
        scene_type = scene.get('scene_type', 'unknown')
        
        # åŸºäºåœºæ™¯å†…å®¹ç”Ÿæˆè§£è¯´
        # è¿™é‡Œå¯ä»¥ç”¨AIï¼Œä½†å…ˆç”¨æ¨¡æ¿ç¡®ä¿åŸºæœ¬å¯ç”¨
        
        narration_templates = {
            'dialogue': [
                f"ç”»é¢ä¸­ï¼Œ{visual}ã€‚",
                f"æ­¤æ—¶ï¼Œ{visual}ã€‚",
                f"é•œå¤´é‡Œï¼Œ{visual}ã€‚",
            ],
            'action': [
                f"ç´§å¼ çš„ä¸€å¹•å‡ºç°äº†ï¼Œ{visual}ã€‚",
                f"ç”»é¢æ€¥è½¬ï¼Œ{visual}ã€‚",
                f"æ­¤åˆ»ï¼Œ{visual}ã€‚",
            ],
            'emotion': [
                f"æƒ…ç»ªè¾¾åˆ°é¡¶ç‚¹ï¼Œ{visual}ã€‚",
                f"ä»¤äººåŠ¨å®¹çš„ä¸€å¹•ï¼Œ{visual}ã€‚",
                f"åœ¨è¿™ä¸€åˆ»ï¼Œ{visual}ã€‚",
            ],
            'transition': [
                f"ç”»é¢ä¸€è½¬ï¼Œ{visual}ã€‚",
                f"é•œå¤´åˆ‡æ¢åˆ°ï¼Œ{visual}ã€‚",
                f"æ¥ä¸‹æ¥ï¼Œ{visual}ã€‚",
            ],
        }
        
        templates = narration_templates.get(scene_type, narration_templates['transition'])
        
        import random
        base_narration = random.choice(templates)
        
        # å¦‚æœæœ‰å¯¹è¯ï¼Œå¯ä»¥æåŠ
        if dialogue and len(dialogue) > 10:
            base_narration += f" {dialogue[:50]}..."
        
        return base_narration


def create_scene_based_timeline(
    analyzed_scenes: List[Dict],
    target_duration: int
) -> List[Dict]:
    """
    åˆ›å»ºåŸºäºåœºæ™¯çš„å‰ªè¾‘æ—¶é—´çº¿
    
    å…³é”®ï¼šè§£è¯´æˆ–åŸå£°äºŒé€‰ä¸€ï¼Œä¸æ··åˆï¼
    """
    timeline = []
    output_cursor = 0.0
    
    for scene in analyzed_scenes:
        if scene['narration_type'] == 'original':
            # åŸå£°åœºæ™¯ï¼šä½¿ç”¨åŸè§†é¢‘éŸ³é¢‘
            audio_mode = 'original'
            narration_text = ''
        else:
            # è§£è¯´åœºæ™¯ï¼šä½¿ç”¨TTSè§£è¯´
            audio_mode = 'voiceover'
            narration_text = scene.get('narration', '')
        
        duration = scene['end_time'] - scene['start_time']
        
        timeline.append({
            'scene_id': scene['scene_id'],
            'source_start': scene['start_time'],
            'source_end': scene['end_time'],
            'output_start': output_cursor,
            'output_end': output_cursor + duration,
            'audio_mode': audio_mode,  # 'original' æˆ– 'voiceover'
            'narration_text': narration_text,
            'visual_content': scene.get('visual_content', ''),
            'importance': scene.get('importance', 0.5),
        })
        
        output_cursor += duration
    
    return timeline


# æµ‹è¯•
if __name__ == "__main__":
    analyzer = VideoContentAnalyzer()
    
    # æ¨¡æ‹Ÿæµ‹è¯•
    test_scenes = [
        {'start': 0, 'end': 30},
        {'start': 100, 'end': 130},
        {'start': 500, 'end': 530},
    ]
    
    test_segments = [
        {'start': 10, 'end': 15, 'text': 'ä½ å¥½å•Š'},
        {'start': 110, 'end': 120, 'text': 'æˆ‘è¦æ€äº†ä½ ï¼'},
    ]
    
    # å¦‚æœæœ‰è§†é¢‘æ–‡ä»¶å¯ä»¥æµ‹è¯•
    # result = analyzer.analyze_video("test.mp4", test_scenes, test_segments)

