# core/cover_generator.py - ç”¨Chinese-CLIPæ‰¾å‡ºæœ€å¸å¼•äººçš„ä¸€å¸§ä½œä¸ºå°é¢
"""
SmartVideoClipper - å°é¢è‡ªåŠ¨ç”Ÿæˆæ¨¡å—

åŠŸèƒ½: ä½¿ç”¨Chinese-CLIPè‡ªåŠ¨é€‰æ‹©æœ€ä½³å°é¢å¸§
ç”¨é€”: ç”Ÿæˆå¸å¼•çœ¼çƒçš„è§†é¢‘å°é¢

ä¾èµ–: cn-clip, torch, opencv-python, pillow
"""

import cv2
from PIL import Image
import numpy as np
import os
import torch

# ğŸ”§ ä½¿ç”¨Chinese-CLIPï¼ˆå›½å†…ç‰ˆï¼‰
CLIP_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

try:
    import cn_clip.clip as clip
    from cn_clip.clip import load_from_name
    # ğŸ”§ é‡å‘½åå˜é‡ï¼Œé¿å…ä¸ç¯å¢ƒå˜é‡CLIP_MODELå†²çª
    _clip_model, _clip_preprocess = load_from_name("ViT-B-16", device=CLIP_DEVICE, download_root='./models')
except ImportError:
    print("âš ï¸ Chinese-CLIPæœªå®‰è£…ï¼Œå°é¢ç”ŸæˆåŠŸèƒ½ä¸å¯ç”¨")
    _clip_model = None
    _clip_preprocess = None


def extract_keyframes(video_path: str, num_frames: int = 50) -> list:
    """
    æå–è§†é¢‘å…³é”®å¸§
    
    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        num_frames: æå–å¸§æ•°
    
    è¿”å›:
        å¸§åˆ—è¡¨ (OpenCVæ ¼å¼ï¼ŒBGR)
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    step = max(total_frames // num_frames, 1)
    
    frames = []
    for i in range(0, total_frames, step):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i)
        ret, frame = cap.read()
        if ret:
            frames.append(frame)
        if len(frames) >= num_frames:
            break
    
    cap.release()
    return frames


def auto_generate_cover(video_path: str, output_path: str):
    """
    è‡ªåŠ¨ç”Ÿæˆè§†é¢‘å°é¢
    ä½¿ç”¨CLIPæ‰¾å‡ºæœ€å¸å¼•äººçš„ä¸€å¸§
    
    å‚æ•°:
        video_path: è§†é¢‘è·¯å¾„
        output_path: å°é¢è¾“å‡ºè·¯å¾„
    """
    if _clip_model is None:
        print("âš ï¸ CLIPæ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•ç”Ÿæˆå°é¢")
        return None
    
    print("ğŸ–¼ï¸ è‡ªåŠ¨ç”Ÿæˆå°é¢...")
    
    # æå–å…³é”®å¸§
    frames = extract_keyframes(video_path, num_frames=50)
    
    if not frames:
        print("âš ï¸ æ— æ³•æå–è§†é¢‘å¸§")
        return None
    
    # å®šä¹‰"å¥½å°é¢"çš„ç‰¹å¾
    prompts = [
        "ç²¾å½©çš„ç”µå½±åœºæ™¯",
        "æˆå‰§æ€§çš„ä¸€å¹•",
        "æ„Ÿäººçš„åœºæ™¯",
        "ç¾ä¸½çš„ç”µå½±ç”»é¢",
        "ä»¤äººå°è±¡æ·±åˆ»çš„é•œå¤´"
    ]
    
    # é¢„è®¡ç®—æ–‡æœ¬ç‰¹å¾
    text_tokens = clip.tokenize(prompts).to(CLIP_DEVICE)
    with torch.no_grad():
        text_features = _clip_model.encode_text(text_tokens)
        text_features /= text_features.norm(dim=-1, keepdim=True)
    
    best_frame, best_score = None, 0
    for frame in frames:
        # å¤„ç†å›¾åƒ
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        image_input = _clip_preprocess(image).unsqueeze(0).to(CLIP_DEVICE)
        
        with torch.no_grad():
            image_features = _clip_model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            score = (image_features @ text_features.T).mean().item()
        
        if score > best_score:
            best_frame, best_score = frame, score
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜å°é¢å›¾ç‰‡
    if best_frame is not None:
        success = cv2.imwrite(output_path, best_frame)
        if success:
            print(f"âœ… å°é¢å·²ä¿å­˜: {output_path} (å¾—åˆ†: {best_score:.3f})")
            return output_path
        else:
            print(f"âš ï¸ å°é¢ä¿å­˜å¤±è´¥: {output_path}")
            return None
    
    return None


def add_title_to_cover(cover_path: str, title: str, output_path: str = None):
    """
    åœ¨å°é¢ä¸Šæ·»åŠ æ ‡é¢˜æ–‡å­—
    
    å‚æ•°:
        cover_path: å°é¢å›¾ç‰‡è·¯å¾„
        title: æ ‡é¢˜æ–‡å­—
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤è¦†ç›–åŸæ–‡ä»¶ï¼‰
    """
    if output_path is None:
        output_path = cover_path
    
    # è¯»å–å›¾ç‰‡
    image = cv2.imread(cover_path)
    if image is None:
        print(f"âš ï¸ æ— æ³•è¯»å–å°é¢: {cover_path}")
        return
    
    h, w = image.shape[:2]
    
    # æ·»åŠ åŠé€æ˜èƒŒæ™¯æ¡
    overlay = image.copy()
    cv2.rectangle(overlay, (0, h - 100), (w, h), (0, 0, 0), -1)
    image = cv2.addWeighted(overlay, 0.5, image, 0.5, 0)
    
    # æ·»åŠ æ–‡å­—
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 1.5
    thickness = 3
    
    # è®¡ç®—æ–‡å­—å¤§å°ä»¥å±…ä¸­
    text_size = cv2.getTextSize(title, font, font_scale, thickness)[0]
    text_x = (w - text_size[0]) // 2
    text_y = h - 40
    
    # ç»˜åˆ¶æ–‡å­—ï¼ˆç™½è‰²å¸¦é»‘è¾¹ï¼‰
    cv2.putText(image, title, (text_x, text_y), font, font_scale, (0, 0, 0), thickness + 2)
    cv2.putText(image, title, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)
    
    success = cv2.imwrite(output_path, image)
    if success:
        print(f"âœ… æ ‡é¢˜å·²æ·»åŠ : {output_path}")
    else:
        print(f"âš ï¸ æ ‡é¢˜ä¿å­˜å¤±è´¥: {output_path}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    test_video = "test_video.mp4"
    
    if os.path.exists(test_video):
        # ç”Ÿæˆå°é¢
        cover = auto_generate_cover(test_video, "cover.jpg")
        
        # æ·»åŠ æ ‡é¢˜
        if cover:
            add_title_to_cover("cover.jpg", "ç²¾å½©è§£è¯´")
    else:
        print(f"âš ï¸ æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {test_video}")
        print("è¯·æä¾›ä¸€ä¸ªè§†é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•")

