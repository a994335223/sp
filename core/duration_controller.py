# core/duration_controller.py - æ™ºèƒ½æ—¶é•¿æ§åˆ¶å™¨
"""
SmartVideoClipper - æ™ºèƒ½æ—¶é•¿æ§åˆ¶

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ ¹æ®å†…å®¹è‡ªåŠ¨å†³å®šè¾“å‡ºæ—¶é•¿
2. è§£è¯´æ–‡æœ¬é•¿åº¦é€‚é…åœºæ™¯æ—¶é•¿
3. ç¡®ä¿è¾“å‡ºåœ¨ç›®æ ‡èŒƒå›´å†…

è®¾è®¡åŸåˆ™ï¼š
- ä¸ç¡¬æ€§è£å‰ªï¼Œè€Œæ˜¯æ™ºèƒ½é€‰æ‹©åœºæ™¯
- è§£è¯´æ–‡æœ¬æ ¹æ®åœºæ™¯æ—¶é•¿è°ƒæ•´
- ä¿è¯å‰§æƒ…è¿è´¯æ€§
"""

from typing import List, Dict, Tuple
import math


class DurationController:
    """
    æ™ºèƒ½æ—¶é•¿æ§åˆ¶å™¨
    
    èŒè´£ï¼š
    1. é€‰æ‹©åœºæ™¯ä»¥è¾¾åˆ°ç›®æ ‡æ—¶é•¿
    2. è°ƒæ•´è§£è¯´æ–‡æœ¬é•¿åº¦
    3. ç¡®ä¿åŸå£°/è§£è¯´æ¯”ä¾‹åˆç†
    """
    
    # è§£è¯´è¯­é€Ÿï¼šçº¦æ¯ç§’4ä¸ªæ±‰å­—
    SPEECH_RATE = 4.0
    
    def __init__(
        self,
        min_duration: int = 180,    # æœ€çŸ­3åˆ†é’Ÿ
        max_duration: int = 900,    # æœ€é•¿15åˆ†é’Ÿ
        original_ratio: float = 0.3  # è‡³å°‘30%åŸå£°
    ):
        self.min_duration = min_duration
        self.max_duration = max_duration
        self.original_ratio = original_ratio
    
    def select_scenes(
        self,
        scenes: List[Dict],
        target_duration: int = None
    ) -> Tuple[List[Dict], int]:
        """
        æ™ºèƒ½é€‰æ‹©åœºæ™¯ä»¥è¾¾åˆ°ç›®æ ‡æ—¶é•¿
        
        å‚æ•°ï¼š
            scenes: æ‰€æœ‰åœºæ™¯åˆ—è¡¨ï¼Œéœ€è¦ start_time, end_time, importance, audio_mode
            target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ ¹æ®å†…å®¹å†³å®šï¼‰
        
        è¿”å›ï¼š
            (é€‰ä¸­çš„åœºæ™¯åˆ—è¡¨, å®é™…æ—¶é•¿)
        """
        if not scenes:
            return [], 0
        
        # è®¡ç®—æ€»å¯ç”¨æ—¶é•¿
        total_available = sum(s['end_time'] - s['start_time'] for s in scenes)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡æ—¶é•¿ï¼Œæ ¹æ®å†…å®¹å†³å®š
        if target_duration is None:
            # é«˜é‡è¦æ€§åœºæ™¯æ—¶é•¿
            high_importance_duration = sum(
                s['end_time'] - s['start_time']
                for s in scenes
                if s.get('importance', 0) >= 0.6
            )
            
            # ç›®æ ‡ = é«˜é‡è¦æ€§ * 1.5ï¼ˆåŠ ä¸Šè¿‡æ¸¡ï¼‰ï¼Œé™åˆ¶åœ¨èŒƒå›´å†…
            target_duration = int(high_importance_duration * 1.5)
            target_duration = max(self.min_duration, min(self.max_duration, target_duration))
        
        print(f"\n[DURATION] æ™ºèƒ½æ—¶é•¿æ§åˆ¶")
        print(f"   æ€»å¯ç”¨: {total_available:.0f}ç§’")
        print(f"   ç›®æ ‡: {target_duration}ç§’ ({target_duration//60}åˆ†{target_duration%60}ç§’)")
        
        # æŒ‰é‡è¦æ€§æ’åº
        sorted_scenes = sorted(scenes, key=lambda x: x.get('importance', 0), reverse=True)
        
        selected = []
        current_duration = 0
        
        # ç¬¬ä¸€è½®ï¼šé€‰æ‹©é«˜é‡è¦æ€§åœºæ™¯ï¼ˆå¿…é¡»ä¿ç•™ï¼‰
        for scene in sorted_scenes:
            if scene.get('importance', 0) >= 0.7:
                duration = scene['end_time'] - scene['start_time']
                if current_duration + duration <= self.max_duration:
                    selected.append(scene)
                    current_duration += duration
        
        # ç¬¬äºŒè½®ï¼šå¡«å……ä¸­ç­‰é‡è¦æ€§åœºæ™¯
        for scene in sorted_scenes:
            if scene in selected:
                continue
            if scene.get('importance', 0) >= 0.4:
                duration = scene['end_time'] - scene['start_time']
                if current_duration + duration <= target_duration:
                    selected.append(scene)
                    current_duration += duration
        
        # ç¬¬ä¸‰è½®ï¼šå¦‚æœè¿˜ä¸å¤Ÿæœ€çŸ­æ—¶é•¿ï¼Œæ·»åŠ æ›´å¤šåœºæ™¯
        if current_duration < self.min_duration:
            for scene in sorted_scenes:
                if scene in selected:
                    continue
                duration = scene['end_time'] - scene['start_time']
                if current_duration + duration <= self.max_duration:
                    selected.append(scene)
                    current_duration += duration
                if current_duration >= self.min_duration:
                    break
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆä¿è¯å‰§æƒ…é¡ºåºï¼‰
        selected.sort(key=lambda x: x['start_time'])
        
        # æ£€æŸ¥åŸå£°æ¯”ä¾‹
        selected = self._ensure_original_ratio(selected)
        
        final_duration = sum(s['end_time'] - s['start_time'] for s in selected)
        
        print(f"   é€‰ä¸­: {len(selected)}ä¸ªåœºæ™¯")
        print(f"   å®é™…: {final_duration:.0f}ç§’ ({final_duration//60:.0f}åˆ†{final_duration%60:.0f}ç§’)")
        
        return selected, int(final_duration)
    
    def _ensure_original_ratio(self, scenes: List[Dict]) -> List[Dict]:
        """ç¡®ä¿åŸå£°æ¯”ä¾‹"""
        original_count = sum(1 for s in scenes if s.get('audio_mode') == 'original')
        total = len(scenes)
        
        if total == 0:
            return scenes
        
        current_ratio = original_count / total
        
        if current_ratio < self.original_ratio:
            # åŸå£°ä¸å¤Ÿï¼Œå°†éƒ¨åˆ†è§£è¯´æ”¹ä¸ºåŸå£°
            need_convert = int(total * self.original_ratio) - original_count
            
            # æŒ‰é‡è¦æ€§æ’åºï¼Œå°†æœ€é‡è¦çš„è§£è¯´åœºæ™¯æ”¹ä¸ºåŸå£°
            voiceover_scenes = [s for s in scenes if s.get('audio_mode') == 'voiceover']
            voiceover_scenes.sort(key=lambda x: x.get('importance', 0), reverse=True)
            
            for i, scene in enumerate(voiceover_scenes):
                if i >= need_convert:
                    break
                scene['audio_mode'] = 'original'
                scene['reason'] = scene.get('reason', '') + ' (å¢åŠ åŸå£°æ¯”ä¾‹)'
        
        return scenes
    
    def adjust_narration_length(
        self,
        narration: str,
        target_duration: float,
        style: str = "å¹½é»˜"
    ) -> str:
        """
        è°ƒæ•´è§£è¯´æ–‡æœ¬é•¿åº¦ä»¥åŒ¹é…åœºæ™¯æ—¶é•¿
        
        å‚æ•°ï¼š
            narration: åŸå§‹è§£è¯´æ–‡æœ¬
            target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰
            style: è§£è¯´é£æ ¼
        
        è¿”å›ï¼š
            è°ƒæ•´åçš„è§£è¯´æ–‡æœ¬
        """
        if not narration:
            return ""
        
        # å½“å‰é¢„ä¼°æ—¶é•¿
        current_chars = len(narration)
        current_duration = current_chars / self.SPEECH_RATE
        
        # ç›®æ ‡å­—æ•°
        target_chars = int(target_duration * self.SPEECH_RATE)
        
        # è°ƒæ•´
        if current_chars > target_chars * 1.3:
            # å¤ªé•¿ï¼Œéœ€è¦ç¼©çŸ­
            # ç®€å•æˆªå–ï¼ˆå®é™…åº”è¯¥ç”¨AIç¼©å†™ï¼‰
            adjusted = narration[:target_chars]
            # ç¡®ä¿ä¸åœ¨ä¸­é—´æ–­å¥
            for punct in ['ã€‚', 'ï¼Œ', 'ï¼', 'ï¼Ÿ', 'ï¼›']:
                last_idx = adjusted.rfind(punct)
                if last_idx > target_chars * 0.7:
                    adjusted = adjusted[:last_idx + 1]
                    break
            return adjusted
        
        elif current_chars < target_chars * 0.5:
            # å¤ªçŸ­ï¼Œä¿æŒåŸæ ·ï¼ˆè§†é¢‘ä¼šæœ‰é™éŸ³ï¼‰
            return narration
        
        else:
            # é•¿åº¦åˆé€‚
            return narration
    
    def create_optimized_timeline(
        self,
        scenes: List[Dict],
        target_duration: int = None
    ) -> List[Dict]:
        """
        åˆ›å»ºä¼˜åŒ–åçš„æ—¶é—´çº¿
        
        è¿™æ˜¯ä¸»å…¥å£å‡½æ•°
        """
        # 1. é€‰æ‹©åœºæ™¯
        selected_scenes, actual_duration = self.select_scenes(scenes, target_duration)
        
        # 2. è°ƒæ•´è§£è¯´é•¿åº¦
        for scene in selected_scenes:
            if scene.get('audio_mode') == 'voiceover' and scene.get('narration'):
                scene_duration = scene['end_time'] - scene['start_time']
                scene['narration'] = self.adjust_narration_length(
                    scene['narration'],
                    scene_duration
                )
        
        # 3. æ„å»ºæ—¶é—´çº¿
        timeline = []
        output_time = 0
        
        for scene in selected_scenes:
            duration = scene['end_time'] - scene['start_time']
            
            timeline.append({
                'scene_id': scene.get('scene_id', len(timeline) + 1),
                'source_start': scene['start_time'],
                'source_end': scene['end_time'],
                'output_start': output_time,
                'output_end': output_time + duration,
                'duration': duration,
                'audio_mode': scene.get('audio_mode', 'original'),
                'narration': scene.get('narration', ''),
                'dialogue': scene.get('dialogue', ''),
                'importance': scene.get('importance', 0.5),
                'emotion': scene.get('emotion', 'neutral'),
                'reason': scene.get('reason', ''),
            })
            
            output_time += duration
        
        # æ‰“å°ç»Ÿè®¡
        orig_count = sum(1 for t in timeline if t['audio_mode'] == 'original')
        voice_count = sum(1 for t in timeline if t['audio_mode'] == 'voiceover')
        
        print(f"\n[TIMELINE] æ—¶é—´çº¿ç”Ÿæˆå®Œæˆ")
        print(f"   ğŸ”Š åŸå£°: {orig_count} ({orig_count*100//(orig_count+voice_count+1)}%)")
        print(f"   ğŸ™ï¸ è§£è¯´: {voice_count} ({voice_count*100//(orig_count+voice_count+1)}%)")
        print(f"   â±ï¸ æ€»æ—¶é•¿: {output_time:.0f}ç§’")
        
        return timeline


def estimate_narration_duration(text: str, speech_rate: float = 4.0) -> float:
    """ä¼°ç®—è§£è¯´æ—¶é•¿"""
    if not text:
        return 0
    return len(text) / speech_rate


# æµ‹è¯•
if __name__ == "__main__":
    controller = DurationController(
        min_duration=180,
        max_duration=600
    )
    
    # æ¨¡æ‹Ÿåœºæ™¯
    test_scenes = [
        {'start_time': 0, 'end_time': 30, 'importance': 0.9, 'audio_mode': 'original'},
        {'start_time': 30, 'end_time': 60, 'importance': 0.5, 'audio_mode': 'voiceover', 'narration': 'è¿™æ˜¯ä¸€æ®µè§£è¯´'},
        {'start_time': 60, 'end_time': 90, 'importance': 0.3, 'audio_mode': 'voiceover'},
        {'start_time': 90, 'end_time': 150, 'importance': 0.8, 'audio_mode': 'original'},
        {'start_time': 150, 'end_time': 200, 'importance': 0.6, 'audio_mode': 'voiceover'},
    ]
    
    timeline = controller.create_optimized_timeline(test_scenes, target_duration=240)
    
    print("\nç”Ÿæˆçš„æ—¶é—´çº¿:")
    for t in timeline:
        mode = "ğŸ”Š" if t['audio_mode'] == 'original' else "ğŸ™ï¸"
        print(f"  {t['source_start']:.0f}s-{t['source_end']:.0f}s {mode} é‡è¦æ€§:{t['importance']:.1f}")

