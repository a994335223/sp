# core/pipeline_v5.py - æ™ºèƒ½è§†é¢‘å‰ªè¾‘æµæ°´çº¿ v5.0 (å·²ä¿®å¤ç‰ˆ)
"""
SmartVideoClipper v5.0 - å…¨çƒç¬¬ä¸€çš„æ™ºèƒ½è§†é¢‘è§£è¯´

å·²ä¿®å¤çš„æ ¸å¿ƒé—®é¢˜ï¼š
1. âœ… éŸ³é¢‘åˆ†æ®µåˆ‡æ¢ï¼ˆæ¯ä¸ªç‰‡æ®µç‹¬ç«‹å¤„ç†ï¼Œä¸æ˜¯å…¨ç¨‹æ··éŸ³ï¼‰
2. âœ… TTSåˆ†æ®µç”Ÿæˆï¼ˆæ¯ä¸ªè§£è¯´åœºæ™¯å•ç‹¬ç”ŸæˆéŸ³é¢‘ï¼‰
3. âœ… è§£è¯´-ç”»é¢æ—¶é•¿å¯¹é½
4. âœ… æ™ºèƒ½æ—¶é•¿æ§åˆ¶
5. âœ… æ•æ„Ÿè¯å¤šå±‚è¿‡æ»¤

å¤„ç†æµç¨‹ï¼š
Step 0: é¢„å¤„ç†ï¼ˆå»ç‰‡å¤´ç‰‡å°¾ï¼‰
Step 1: è¯­éŸ³è¯†åˆ«ï¼ˆè·å–å¯¹è¯ï¼‰
Step 2: åœºæ™¯åˆ†æï¼ˆæ ‡è®°ç²¾å½©/è¿‡æ¸¡ï¼‰
Step 3: æ™ºèƒ½è§£è¯´ï¼ˆç”Ÿæˆæ–‡æ¡ˆï¼‰
Step 4: æ—¶é•¿æ§åˆ¶ï¼ˆé€‰æ‹©åœºæ™¯ï¼‰
Step 5: TTSåˆ†æ®µåˆæˆ
Step 6: ç‰‡æ®µå¤„ç†ï¼ˆåŸå£°/è§£è¯´åˆ†å¼€ï¼‰
Step 7: è¾“å‡ºæˆå“
"""

import os
import sys
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "core"))

# ç¯å¢ƒé…ç½®
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# GPUåŠ é€Ÿæ£€æµ‹
try:
    from gpu_encoder import get_encoder, is_hardware_available
    GPU_ENCODER = get_encoder()
except ImportError:
    GPU_ENCODER = None
    def is_hardware_available():
        return False


PROCESS_STEPS_V5 = {
    0: "é¢„å¤„ç†",
    1: "è¯­éŸ³è¯†åˆ«", 
    2: "åœºæ™¯åˆ†æ",
    3: "æ™ºèƒ½è§£è¯´",
    4: "æ—¶é•¿æ§åˆ¶",
    5: "TTSåˆ†æ®µåˆæˆ",
    6: "ç‰‡æ®µå¤„ç†",
    7: "è¾“å‡ºæˆå“",
}


class VideoPipelineV5:
    """
    SmartVideoClipper v5.0 å¤„ç†æµæ°´çº¿ (å·²ä¿®å¤ç‰ˆ)
    
    æ ¸å¿ƒæ”¹è¿›ï¼š
    - æ¯ä¸ªç‰‡æ®µç‹¬ç«‹å¤„ç†éŸ³é¢‘ï¼ˆä¸æ˜¯å…¨ç¨‹æ··éŸ³ï¼‰
    - æ¯ä¸ªè§£è¯´åœºæ™¯å•ç‹¬ç”ŸæˆTTS
    - è§£è¯´æ—¶é•¿è‡ªåŠ¨é€‚é…åœºæ™¯æ—¶é•¿
    - æ™ºèƒ½é€‰æ‹©åœºæ™¯ä»¥è¾¾åˆ°ç›®æ ‡æ—¶é•¿
    """
    
    def __init__(self):
        self.start_time = None
        self.work_dir = None
        
    async def process(
        self,
        video_path: str,
        output_name: str,
        title: str = "",
        style: str = "å¹½é»˜",
        min_duration: int = 180,   # æœ€çŸ­3åˆ†é’Ÿ
        max_duration: int = 900,   # æœ€é•¿15åˆ†é’Ÿ
        media_type: str = "auto",  # auto/tv/movie
        episode: int = 0,          # ç¬¬å‡ é›†/éƒ¨ï¼ˆ0=è‡ªåŠ¨æ£€æµ‹ï¼‰
        progress_callback=None
    ) -> Dict:
        """
        å¤„ç†è§†é¢‘
        
        å‚æ•°ï¼š
            video_path: è§†é¢‘è·¯å¾„
            output_name: è¾“å‡ºåç§°
            title: ä½œå“åç§°
            style: è§£è¯´é£æ ¼
            min_duration: æœ€çŸ­æ—¶é•¿ï¼ˆç§’ï¼‰
            max_duration: æœ€é•¿æ—¶é•¿ï¼ˆç§’ï¼‰
            media_type: åª’ä½“ç±»å‹ (autoè‡ªåŠ¨/tvç”µè§†å‰§/movieç”µå½±)
            episode: é›†æ•°/éƒ¨æ•° (0è¡¨ç¤ºè‡ªåŠ¨ä»æ–‡ä»¶åè§£æ)
            progress_callback: è¿›åº¦å›è°ƒ
        """
        self.start_time = datetime.now()
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        self.work_dir = project_root / f"workspace_{output_name}"
        self.work_dir.mkdir(exist_ok=True)
        
        def report_progress(step: int, message: str):
            """æŠ¥å‘Šè¿›åº¦"""
            elapsed = (datetime.now() - self.start_time).seconds
            pct = int(step / 8 * 100)
            
            print(f"\n{'='*60}")
            print(f"[{datetime.now().strftime('%H:%M:%S')}] è¿›åº¦: {'â–ˆ'*(pct//3)}{'â–‘'*(33-pct//3)} {pct}%")
            print(f"[{datetime.now().strftime('%H:%M:%S')}] æ­¥éª¤ {step}/8: {PROCESS_STEPS_V5.get(step, 'æœªçŸ¥')}")
            print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
            print(f"[{datetime.now().strftime('%H:%M:%S')}] å·²è€—æ—¶: {elapsed}ç§’")
            print(f"{'='*60}")
            
            if progress_callback:
                progress_callback(step, message, pct)
        
        # è‡ªåŠ¨æ£€æµ‹åª’ä½“ç±»å‹å’Œé›†æ•°
        from plot_fetcher import parse_episode_from_filename, extract_title_from_filename
        
        if not title:
            title = extract_title_from_filename(video_path)
        
        # è‡ªåŠ¨è§£æé›†æ•°
        auto_season, auto_episode = parse_episode_from_filename(video_path)
        if episode == 0:
            episode = auto_episode
        
        # è‡ªåŠ¨åˆ¤æ–­åª’ä½“ç±»å‹ï¼ˆæœ‰é›†æ•°æ ‡è®° â†’ ç”µè§†å‰§ï¼‰
        if media_type == "auto":
            if auto_episode > 1 or "E0" in video_path.upper() or "ç¬¬" in video_path and "é›†" in video_path:
                media_type = "tv"
            else:
                media_type = "movie"  # é»˜è®¤ç”µå½±
        
        # æ‰“å°å¯åŠ¨ä¿¡æ¯
        print("\n" + "="*60)
        print("[PIPELINE] SmartVideoClipper v5.1 - ç”µå½±/ç”µè§†å‰§åˆ†ç¦»ç‰ˆ")
        print("="*60)
        print("   æ ¸å¿ƒå‡çº§:")
        print("   1. [OK] ç”µå½±/ç”µè§†å‰§æ¨¡å¼åˆ†ç¦»ï¼ˆè§£è¯´ç­–ç•¥ä¸åŒï¼‰")
        print("   2. [OK] éŸ³é¢‘åˆ†æ®µåˆ‡æ¢ï¼ˆæ¯ä¸ªç‰‡æ®µç‹¬ç«‹å¤„ç†ï¼‰")
        print("   3. [OK] TTSåˆ†æ®µç”Ÿæˆï¼ˆè§£è¯´-ç”»é¢ç²¾ç¡®å¯¹é½ï¼‰")
        print("   4. [OK] æ™ºèƒ½æ—¶é•¿æ§åˆ¶")
        print("   5. [OK] GPUç¡¬ä»¶åŠ é€Ÿç¼–ç ")
        print("="*60)
        
        # GPUåŠ é€ŸçŠ¶æ€
        if GPU_ENCODER:
            gpu_info = GPU_ENCODER.get_info()
            if gpu_info['is_hardware']:
                print(f"   [GPU] åŠ é€Ÿ: {gpu_info['name']} (10xé€Ÿæå‡!)")
            else:
                print(f"   [GPU] ä¸å¯ç”¨ï¼Œä½¿ç”¨CPUç¼–ç ")
        else:
            print(f"   [GPU] æ£€æµ‹æ¨¡å—æœªåŠ è½½")
        
        print("="*60)
        print(f"   å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   è¾“å…¥è§†é¢‘: {video_path}")
        print(f"   ä½œå“åç§°: {title}")
        media_type_cn = "ç”µè§†å‰§" if media_type == "tv" else "ç”µå½±"
        print(f"   åª’ä½“ç±»å‹: {media_type_cn}")
        if media_type == "tv":
            print(f"   å½“å‰é›†æ•°: ç¬¬{episode}é›†")
            print(f"   è§£è¯´ç­–ç•¥: è®²è¿°æœ¬é›†æ•…äº‹ï¼ˆ60%è§£è¯´+40%åŸå£°ï¼‰")
        else:
            print(f"   å½“å‰éƒ¨æ•°: ç¬¬{episode}éƒ¨")
            print(f"   è§£è¯´ç­–ç•¥: ç²¾å½©ç‰‡æ®µé›†é”¦ï¼ˆ40%è§£è¯´+60%åŸå£°ï¼‰")
        print(f"   è§£è¯´é£æ ¼: {style}")
        print(f"   æ—¶é•¿èŒƒå›´: {min_duration//60}-{max_duration//60}åˆ†é’Ÿ")
        print("="*60 + "\n")
        
        try:
            # ========== Step 0: é¢„å¤„ç† ==========
            report_progress(0, "æ£€æµ‹å¹¶å»é™¤ç‰‡å¤´ç‰‡å°¾...")
            
            from intro_outro_detect import auto_trim_intro_outro
            # è¿”å›å€¼: (è¾“å‡ºè·¯å¾„, ç‰‡å¤´ç»“æŸæ—¶é—´, ç‰‡å°¾å¼€å§‹æ—¶é—´)
            trim_result = auto_trim_intro_outro(video_path, str(self.work_dir))
            
            if isinstance(trim_result, tuple):
                processed_video = trim_result[0]
            else:
                processed_video = trim_result
            
            if not processed_video or not os.path.exists(processed_video):
                processed_video = video_path
                print("   [INFO] æ— éœ€è£å‰ªï¼Œä½¿ç”¨åŸè§†é¢‘")
            
            # ========== Step 1: è¯­éŸ³è¯†åˆ« ==========
            report_progress(1, "è¯†åˆ«è§†é¢‘ä¸­çš„å¯¹è¯...")
            
            from transcribe import transcribe_video
            srt_path = str(self.work_dir / "subtitles.srt")
            segments, full_text = transcribe_video(processed_video, output_srt=srt_path)
            
            print(f"   âœ“ è¯†åˆ«åˆ° {len(segments)} æ®µå¯¹è¯")
            
            # ========== Step 2: åœºæ™¯åˆ†æ ==========
            report_progress(2, "åˆ†æè§†é¢‘åœºæ™¯...")
            
            from scene_detect import detect_scenes
            from smart_importance import calculate_scene_importance
            from plot_fetcher import PlotFetcher
            
            # è·å–å‰§æƒ…ä¿¡æ¯ï¼ˆç”µè§†å‰§ï¼šè·å–åˆ†é›†å‰§æƒ…ï¼‰
            plot_fetcher = PlotFetcher()
            plot_info = plot_fetcher.fetch(
                title=title,
                media_type=media_type,
                season=auto_season,
                episode=episode
            )
            plot_fetcher.close()
            
            # æå–åˆ†é›†å‰§æƒ…ï¼ˆç”¨äºè§£è¯´å¼•æ“ï¼‰
            episode_plot = ""
            if media_type == "tv":
                episode_plot = plot_info.get('episode_overview', '') or plot_info.get('overview', '')
                if episode_plot:
                    print(f"   [å‰§æƒ…] ç¬¬{episode}é›†å‰§æƒ…: {episode_plot[:80]}...")
                else:
                    # ä½¿ç”¨AIä»å­—å¹•æ€»ç»“æœ¬é›†å‰§æƒ…
                    from plot_fetcher import summarize_plot_from_transcript
                    episode_plot = summarize_plot_from_transcript(full_text, segments)
                    if episode_plot:
                        print(f"   [å‰§æƒ…] AIæ€»ç»“æœ¬é›†å‰§æƒ…: {episode_plot[:80]}...")
            
            # æ£€æµ‹åœºæ™¯
            scenes_dir = str(self.work_dir / "scenes")
            raw_scenes, _ = detect_scenes(processed_video, scenes_dir)  # è§£åŒ…å…ƒç»„
            print(f"   æ£€æµ‹åˆ° {len(raw_scenes)} ä¸ªåœºæ™¯")
            
            # è®¡ç®—é‡è¦æ€§å¹¶å…³è”å¯¹è¯
            analyzed_scenes = []
            for i, scene in enumerate(raw_scenes):
                scene_start = scene['start']  # ä¿®æ­£é”®å
                scene_end = scene['end']      # ä¿®æ­£é”®å
                
                # æ‰¾åˆ°è¯¥åœºæ™¯å†…çš„å¯¹è¯
                scene_dialogue = ""
                for seg in segments:
                    if seg['start'] >= scene_start and seg['end'] <= scene_end:
                        scene_dialogue += seg['text'] + " "
                    elif seg['start'] < scene_end and seg['end'] > scene_start:
                        scene_dialogue += seg['text'] + " "
                
                scene_dialogue = scene_dialogue.strip()
                
                # æ£€æµ‹æƒ…æ„Ÿ
                emotion = self._detect_emotion(scene_dialogue)
                
                # è®¡ç®—é‡è¦æ€§
                importance = calculate_scene_importance(
                    scene_dialogue, 
                    scene_end - scene_start,
                    emotion
                )
                
                analyzed_scenes.append({
                    'scene_id': i + 1,
                    'start_time': scene_start,
                    'end_time': scene_end,
                    'dialogue': scene_dialogue,
                    'emotion': emotion,
                    'importance': importance,
                })
            
            print(f"   âœ“ åœºæ™¯åˆ†æå®Œæˆ")
            
            # ========== Step 3: æ™ºèƒ½è§£è¯´ ==========
            report_progress(3, f"ç”Ÿæˆ{style}é£æ ¼è§£è¯´ï¼ˆ{media_type_cn}æ¨¡å¼ï¼‰...")
            
            from narration_engine import NarrationEngine
            
            # åˆå§‹åŒ–è§£è¯´å¼•æ“ï¼ˆä¼ å…¥åª’ä½“ç±»å‹å’Œé›†æ•°ï¼‰
            engine = NarrationEngine(
                use_ai=True, 
                media_type=media_type, 
                episode=episode
            )
            scene_segments, narration_text = engine.analyze_and_generate(
                analyzed_scenes, 
                title, 
                style,
                episode_plot=episode_plot  # ä¼ å…¥åˆ†é›†å‰§æƒ…
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            scenes_with_narration = []
            for seg in scene_segments:
                scenes_with_narration.append({
                    'scene_id': seg.scene_id,
                    'start_time': seg.start_time,
                    'end_time': seg.end_time,
                    'dialogue': seg.dialogue,
                    'narration': seg.narration,
                    'audio_mode': seg.audio_mode.value,  # è½¬ä¸ºå­—ç¬¦ä¸²
                    'importance': seg.importance,
                    'emotion': seg.emotion,
                    'reason': seg.reason,
                })
            
            print(f"   âœ“ è§£è¯´ç”Ÿæˆå®Œæˆ")
            
            # ========== Step 4: æ—¶é•¿æ§åˆ¶ ==========
            report_progress(4, "æ™ºèƒ½é€‰æ‹©åœºæ™¯...")
            
            from duration_controller import DurationController
            
            controller = DurationController(
                min_duration=min_duration,
                max_duration=max_duration,
                original_ratio=0.3  # è‡³å°‘30%åŸå£°
            )
            
            timeline = controller.create_optimized_timeline(
                scenes_with_narration,
                target_duration=None  # è‡ªåŠ¨è®¡ç®—
            )
            
            # è¿‡æ»¤è·³è¿‡çš„åœºæ™¯
            active_timeline = [t for t in timeline if t['audio_mode'] != 'skip']
            
            if not active_timeline:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„åœºæ™¯")
            
            total_duration = sum(t['duration'] for t in active_timeline)
            
            print(f"   âœ“ é€‰æ‹©äº† {len(active_timeline)} ä¸ªåœºæ™¯")
            print(f"   é¢„è®¡æ—¶é•¿: {total_duration:.0f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
            
            # ä¿å­˜è§£è¯´å‰§æœ¬
            script_path = self.work_dir / "è§£è¯´å‰§æœ¬_v5.txt"
            self._save_script(active_timeline, script_path, title, style)
            
            # ========== Step 5: TTSåˆ†æ®µåˆæˆ ==========
            report_progress(5, "åˆ†æ®µåˆæˆè§£è¯´é…éŸ³...")
            
            from tts_segmented import synthesize_timeline_narrations
            
            tts_dir = self.work_dir / "tts"
            narration_segments = await synthesize_timeline_narrations(
                active_timeline,
                str(tts_dir)
            )
            
            print(f"   âœ“ ç”Ÿæˆ {len(narration_segments)} ä¸ªè§£è¯´éŸ³é¢‘")
            
            # ========== Step 6: ç‰‡æ®µå¤„ç† ==========
            report_progress(6, "å¤„ç†è§†é¢‘ç‰‡æ®µï¼ˆåŸå£°/è§£è¯´åˆ†å¼€ï¼‰...")
            
            from clip_processor import process_timeline_clips, concat_processed_clips
            
            clips_dir = self.work_dir / "clips"
            
            # å¤„ç†æ¯ä¸ªç‰‡æ®µï¼ˆå…³é”®æ”¹è¿›ï¼šæ¯ä¸ªç‰‡æ®µç‹¬ç«‹å¤„ç†éŸ³é¢‘ï¼‰
            clip_files, clips_duration = process_timeline_clips(
                source_video=processed_video,
                timeline=active_timeline,
                narration_segments=narration_segments,
                output_dir=str(clips_dir)
            )
            
            # æ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
            output_video = str(self.work_dir / f"{output_name}.mp4")
            
            if not clip_files:
                raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•è§†é¢‘ç‰‡æ®µ")
            
            concat_success = concat_processed_clips(clip_files, output_video)
            if not concat_success:
                raise RuntimeError("è§†é¢‘ç‰‡æ®µæ‹¼æ¥å¤±è´¥")
            
            print(f"   âœ“ è§†é¢‘å¤„ç†å®Œæˆ")
            
            # ========== Step 7: è¾“å‡ºæˆå“ ==========
            report_progress(7, "ç”Ÿæˆæœ€ç»ˆæˆå“...")
            
            from audio_composer import add_subtitles, convert_to_vertical
            
            # æ·»åŠ å­—å¹•
            output_with_sub = str(self.work_dir / f"{output_name}_sub.mp4")
            add_subtitles(output_video, srt_path, output_with_sub)
            
            # ç”ŸæˆæŠ–éŸ³ç‰ˆ
            output_douyin = str(self.work_dir / f"{output_name}_æŠ–éŸ³.mp4")
            convert_to_vertical(output_video, output_douyin)
            
            # å®Œæˆ
            end_time = datetime.now()
            elapsed = (end_time - self.start_time).seconds
            
            print("\n" + "â˜…"*60)
            print("â˜…  âœ… v5.0 å¤„ç†å®Œæˆï¼")
            print("â˜…  ====================================================")
            print(f"â˜…  ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â˜…  æ€»è€—æ—¶: {elapsed//60}åˆ†{elapsed%60}ç§’")
            print(f"â˜…  è¾“å‡ºæ–‡ä»¶: {output_video}")
            print("â˜…"*60 + "\n")
            
            # ç»Ÿè®¡
            orig_count = sum(1 for t in active_timeline if t['audio_mode'] == 'original')
            voice_count = sum(1 for t in active_timeline if t['audio_mode'] == 'voiceover')
            
            return {
                'success': True,
                'output_video': output_video,
                'output_douyin': output_douyin,
                'output_with_subtitle': output_with_sub,
                'script_path': str(script_path),
                'subtitle_path': srt_path,
                'duration': total_duration,
                'original_scenes': orig_count,
                'voiceover_scenes': voice_count,
                'work_dir': str(self.work_dir),
            }
            
        except Exception as e:
            print(f"\n[ERROR] å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
            }
    
    def _detect_emotion(self, dialogue: str) -> str:
        """æ£€æµ‹å¯¹è¯æƒ…æ„Ÿ"""
        if not dialogue:
            return 'neutral'
        
        emotion_keywords = {
            'angry': ['æ»š', 'æ··è›‹', 'å¦ˆçš„', 'æ“', 'æ€', 'æ‰“', 'æ', 'æ„¤æ€’', 'ç”Ÿæ°”', 'å»æ­»'],
            'sad': ['å“­', 'æ³ª', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'ç—›è‹¦', 'å¯¹ä¸èµ·', 'æŠ±æ­‰', 'æ‚²ä¼¤', 'æ­»äº†'],
            'happy': ['å“ˆå“ˆ', 'å¼€å¿ƒ', 'é«˜å…´', 'å¤ªå¥½äº†', 'æ£’', 'èµ', 'ç¬‘', 'å“ˆ'],
            'fear': ['æ€•', 'å®³æ€•', 'ææƒ§', 'å¯æ€•', 'å“', 'æƒŠ', 'æ•‘å‘½'],
            'excited': ['æ¿€åŠ¨', 'å…´å¥‹', 'å¤ªæ£’äº†', 'ä¸æ•¢ç›¸ä¿¡', 'å¤©å“ª'],
        }
        
        for emotion, keywords in emotion_keywords.items():
            if any(kw in dialogue for kw in keywords):
                return emotion
        
        return 'neutral'
    
    def _save_script(self, timeline: List[Dict], path: Path, title: str, style: str):
        """ä¿å­˜è§£è¯´å‰§æœ¬"""
        lines = []
        lines.append("=" * 60)
        lines.append(f"SmartVideoClipper v5.0 - è§£è¯´å‰§æœ¬ (å·²ä¿®å¤ç‰ˆ)")
        lines.append(f"ä½œå“: {title}")
        lines.append(f"é£æ ¼: {style}")
        lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("=" * 60)
        lines.append("")
        lines.append("æ ¸å¿ƒæ”¹è¿›ï¼š")
        lines.append("  1. æ¯ä¸ªç‰‡æ®µç‹¬ç«‹å¤„ç†éŸ³é¢‘ï¼ˆåŸå£°/è§£è¯´åˆ†å¼€ï¼‰")
        lines.append("  2. TTSåˆ†æ®µç”Ÿæˆï¼ˆè§£è¯´-ç”»é¢ç²¾ç¡®å¯¹é½ï¼‰")
        lines.append("  3. æ™ºèƒ½æ—¶é•¿æ§åˆ¶")
        lines.append("")
        lines.append("=" * 60)
        lines.append("")
        
        for i, item in enumerate(timeline, 1):
            mode = "ğŸ”ŠåŸå£°" if item['audio_mode'] == 'original' else "ğŸ™ï¸è§£è¯´"
            lines.append(f"ã€åœºæ™¯ {i}ã€‘ {mode}")
            lines.append(f"æ—¶é—´: {item['source_start']:.1f}s - {item['source_end']:.1f}s ({item['duration']:.1f}ç§’)")
            lines.append(f"é‡è¦æ€§: {item['importance']:.2f}")
            
            if item.get('dialogue'):
                lines.append(f"å¯¹ç™½: {item['dialogue'][:100]}...")
            
            if item.get('narration') and item['audio_mode'] == 'voiceover':
                lines.append(f"è§£è¯´: {item['narration']}")
            
            if item.get('reason'):
                lines.append(f"åŸå› : {item['reason']}")
            
            lines.append("")
        
        # ç»Ÿè®¡
        orig_count = sum(1 for t in timeline if t['audio_mode'] == 'original')
        voice_count = sum(1 for t in timeline if t['audio_mode'] == 'voiceover')
        total_duration = sum(t['duration'] for t in timeline)
        
        lines.append("=" * 60)
        lines.append("ç»Ÿè®¡:")
        lines.append(f"  æ€»åœºæ™¯: {len(timeline)}")
        lines.append(f"  åŸå£°åœºæ™¯: {orig_count} ({orig_count*100//(orig_count+voice_count+1)}%)")
        lines.append(f"  è§£è¯´åœºæ™¯: {voice_count} ({voice_count*100//(orig_count+voice_count+1)}%)")
        lines.append(f"  æ€»æ—¶é•¿: {total_duration:.0f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
        lines.append("=" * 60)
        
        with open(path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        
        print(f"   âœ“ å‰§æœ¬å·²ä¿å­˜: {path}")


async def run_v5(
    video_path: str,
    output_name: str,
    title: str = "",
    style: str = "å¹½é»˜",
    min_duration: int = 180,
    max_duration: int = 900,
    media_type: str = "auto",  # ğŸ†• åª’ä½“ç±»å‹ (auto/tv/movie)
    episode: int = 0           # ğŸ†• é›†æ•°/éƒ¨æ•°
) -> Dict:
    """
    è¿è¡Œ v5.1 æµæ°´çº¿
    
    è¿™æ˜¯å¯¹å¤–çš„ä¸»å…¥å£
    
    å‚æ•°ï¼š
        video_path: è§†é¢‘è·¯å¾„
        output_name: è¾“å‡ºåç§°
        title: ä½œå“åç§°
        style: è§£è¯´é£æ ¼
        min_duration: æœ€çŸ­æ—¶é•¿ï¼ˆç§’ï¼‰
        max_duration: æœ€é•¿æ—¶é•¿ï¼ˆç§’ï¼‰
        media_type: åª’ä½“ç±»å‹ (autoè‡ªåŠ¨/tvç”µè§†å‰§/movieç”µå½±)
        episode: é›†æ•°/éƒ¨æ•°ï¼ˆ0=è‡ªåŠ¨ä»æ–‡ä»¶åè§£æï¼‰
    """
    pipeline = VideoPipelineV5()
    return await pipeline.process(
        video_path=video_path,
        output_name=output_name,
        title=title,
        style=style,
        min_duration=min_duration,
        max_duration=max_duration,
        media_type=media_type,
        episode=episode
    )


# æµ‹è¯•å…¥å£
if __name__ == "__main__":
    import asyncio
    
    async def test():
        result = await run_v5(
            video_path=r"C:\Users\Administrator\Downloads\ç‹‚é£™E01.mp4",
            output_name="ç‹‚é£™ç¬¬ä¸€é›†_v5",
            title="ç‹‚é£™",
            style="å¹½é»˜"
        )
        print(result)
    
    asyncio.run(test())
