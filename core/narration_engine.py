# core/narration_engine.py - è§£è¯´å¼•æ“ v5.1 (ç”µå½±/ç”µè§†å‰§åˆ†ç¦»ç‰ˆ)
"""
SmartVideoClipper - æ™ºèƒ½è§£è¯´å¼•æ“ v5.1

ğŸ¬ æ ¸å¿ƒå‡çº§ï¼šç”µå½±ä¸ç”µè§†å‰§æ¨¡å¼åˆ†ç¦»

ç”µè§†å‰§æ¨¡å¼ï¼ˆTVï¼‰ï¼š
- éœ€è¦æŒ‡å®šç¬¬å‡ é›†
- è§£è¯´èšç„¦"å½“å‰é›†"å‰§æƒ…ï¼Œä¸æ˜¯æ•´éƒ¨å‰§
- è§£è¯´æ¯”ä¾‹æ›´é«˜ï¼ˆ60%ï¼‰ï¼Œè®©è§‚ä¼—å¿«é€Ÿäº†è§£æœ¬é›†å†…å®¹
- é€‚åˆ"3åˆ†é’Ÿçœ‹å®Œä¸€é›†"çš„è§£è¯´é£æ ¼

ç”µå½±æ¨¡å¼ï¼ˆMovieï¼‰ï¼š
- å¯æŒ‡å®šç³»åˆ—ç”µå½±ç¬¬å‡ éƒ¨
- è§£è¯´æ¶µç›–æ•´ä½“å‰§æƒ…è„‰ç»œ
- åŸå£°æ¯”ä¾‹æ›´é«˜ï¼ˆ60%ï¼‰ï¼Œä¿ç•™ç»å…¸å°è¯
- é€‚åˆ"ç²¾å½©ç‰‡æ®µé›†é”¦"é£æ ¼

ä¸‰ç§éŸ³é¢‘æ¨¡å¼ï¼š
- ğŸ”Š åŸå£°åœºæ™¯ï¼šç²¾å½©å¯¹è¯ã€æƒ…æ„Ÿçˆ†å‘ã€åŠ¨ä½œé«˜æ½®
- ğŸ™ï¸ è§£è¯´åœºæ™¯ï¼šè¿‡æ¸¡ã€èƒŒæ™¯äº¤ä»£ã€å¿«è¿›
- ğŸ”‡ è·³è¿‡åœºæ™¯ï¼šæ— æ„ä¹‰ã€é‡å¤ã€æ‹–æ²“
"""

import os
import sys
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

# æ•æ„Ÿè¯åˆ—è¡¨
SENSITIVE_WORDS = [
    "ä¹ è¿‘å¹³", "èƒ¡é”¦æ¶›", "æ±Ÿæ³½æ°‘", "æ¯›æ³½ä¸œ", "é‚“å°å¹³", "æ¸©å®¶å®", "æå…‹å¼º",
    "ä¹ ä¸»å¸­", "æ€»ä¹¦è®°", "å›½å®¶ä¸»å¸­", "ä¸­å¤®é¢†å¯¼", "å…±äº§å…š", "å›½æ°‘å…š", 
    "æ°‘è¿›å…š", "æ³•è½®åŠŸ", "å…­å››", "å¤©å®‰é—¨", "å°ç‹¬", "è—ç‹¬", "ç–†ç‹¬", "æ¸¯ç‹¬",
]

# ä½è´¨é‡å†…å®¹æ£€æµ‹
BAD_PATTERNS = [
    "ç´§å¼ çš„åœºé¢", "ç´§å¼ çš„ä¸€å¹•", "æ­¤åˆ»ç´§å¼ ", "ç”»é¢ä¸€è½¬ï¼Œç´§å¼ ",
    "æœªçŸ¥åœºæ™¯", "unknown", "åœºæ™¯1", "åœºæ™¯2",
]


class AudioMode(Enum):
    ORIGINAL = "original"    # ä¿ç•™åŸå£°
    VOICEOVER = "voiceover"  # ä½¿ç”¨è§£è¯´
    SKIP = "skip"            # è·³è¿‡


@dataclass
class SceneSegment:
    """åœºæ™¯ç‰‡æ®µ"""
    scene_id: int
    start_time: float
    end_time: float
    dialogue: str           # åŸå§‹å¯¹è¯
    narration: str          # ç”Ÿæˆçš„è§£è¯´ï¼ˆå¦‚æœéœ€è¦ï¼‰
    audio_mode: AudioMode   # éŸ³é¢‘æ¨¡å¼
    importance: float       # é‡è¦æ€§åˆ†æ•°
    emotion: str            # æƒ…æ„Ÿ
    reason: str             # é€‰æ‹©åŸå› ï¼ˆè°ƒè¯•ç”¨ï¼‰
    
    @property
    def duration(self) -> float:
        return self.end_time - self.start_time


class NarrationEngine:
    """
    æ™ºèƒ½è§£è¯´å¼•æ“ v5.1
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. æ ¹æ®åª’ä½“ç±»å‹ï¼ˆç”µå½±/ç”µè§†å‰§ï¼‰é€‰æ‹©ä¸åŒç­–ç•¥
    2. åˆ†æåœºæ™¯ï¼Œå†³å®šåŸå£°/è§£è¯´/è·³è¿‡
    3. ä¸ºè§£è¯´åœºæ™¯ç”Ÿæˆé«˜è´¨é‡æ–‡æ¡ˆ
    4. ç¡®ä¿å‰§æƒ…è¿è´¯æ€§
    """
    
    def __init__(self, use_ai: bool = True, media_type: str = "tv", episode: int = 1):
        """
        åˆå§‹åŒ–è§£è¯´å¼•æ“
        
        å‚æ•°ï¼š
            use_ai: æ˜¯å¦ä½¿ç”¨AIç”Ÿæˆ
            media_type: åª’ä½“ç±»å‹ ("tv" ç”µè§†å‰§, "movie" ç”µå½±)
            episode: é›†æ•°/éƒ¨æ•° (ç”µè§†å‰§ç¬¬å‡ é›† æˆ– ç”µå½±ç¬¬å‡ éƒ¨)
        """
        self.use_ai = use_ai
        self.llm_model = None
        self.media_type = media_type
        self.episode = episode
        
        # æ ¹æ®åª’ä½“ç±»å‹è®¾ç½®ç­–ç•¥
        if media_type == "tv":
            # ç”µè§†å‰§ï¼šæ›´å¤šè§£è¯´ï¼Œè®²å½“å‰é›†çš„æ•…äº‹
            self.voiceover_ratio = 0.6  # 60%è§£è¯´
            self.min_original_ratio = 0.25  # æœ€å°‘25%åŸå£°
        else:
            # ç”µå½±ï¼šæ›´å¤šåŸå£°ï¼Œä¿ç•™ç»å…¸å°è¯
            self.voiceover_ratio = 0.4  # 40%è§£è¯´
            self.min_original_ratio = 0.45  # æœ€å°‘45%åŸå£°
        
        # å°è¯•åŠ è½½LLM
        if use_ai:
            self._init_llm()
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLMæ¨¡å‹"""
        try:
            import ollama
            models = ollama.list()
            
            # è·å–å¯ç”¨æ¨¡å‹
            available = []
            for model in models.get('models', []):
                name = model.get('name', '') or model.get('model', '')
                if name:
                    available.append(name.split(':')[0])
            
            # ä¼˜å…ˆçº§é€‰æ‹©
            priority = ['qwen3', 'qwen2.5', 'qwen', 'llama3', 'gemma']
            for p in priority:
                for a in available:
                    if p in a.lower():
                        self.llm_model = a
                        print(f"[LLM] ä½¿ç”¨æ¨¡å‹: {self.llm_model}")
                        return
            
            if available:
                self.llm_model = available[0]
                print(f"[LLM] ä½¿ç”¨æ¨¡å‹: {self.llm_model}")
        except Exception as e:
            print(f"[LLM] åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm_model = None
    
    def analyze_and_generate(
        self,
        scenes: List[Dict],
        title: str = "",
        style: str = "å¹½é»˜",
        episode_plot: str = ""
    ) -> Tuple[List[SceneSegment], str]:
        """
        åˆ†æåœºæ™¯å¹¶ç”Ÿæˆè§£è¯´
        
        å‚æ•°ï¼š
            scenes: åœºæ™¯åˆ—è¡¨
            title: ä½œå“åç§°
            style: è§£è¯´é£æ ¼
            episode_plot: åˆ†é›†å‰§æƒ…ï¼ˆç”µè§†å‰§ç”¨ï¼‰
        
        è¿”å›ï¼š(å¤„ç†åçš„åœºæ™¯åˆ—è¡¨, å®Œæ•´è§£è¯´æ–‡æœ¬)
        """
        print("\n" + "="*60)
        print("[Engine] æ™ºèƒ½è§£è¯´å¼•æ“ v5.1")
        print("="*60)
        print(f"   ä½œå“: {title}")
        print(f"   ç±»å‹: {'ç”µè§†å‰§' if self.media_type == 'tv' else 'ç”µå½±'}")
        if self.media_type == "tv":
            print(f"   é›†æ•°: ç¬¬{self.episode}é›†")
        else:
            print(f"   éƒ¨æ•°: ç¬¬{self.episode}éƒ¨")
        print(f"   é£æ ¼: {style}")
        print(f"   åœºæ™¯æ•°: {len(scenes)}")
        print(f"   è§£è¯´æ¯”ä¾‹ç›®æ ‡: {self.voiceover_ratio*100:.0f}%")
        print("="*60)
        
        # ä¿å­˜åˆ†é›†å‰§æƒ…ä¾›åç»­ä½¿ç”¨
        self.episode_plot = episode_plot
        
        # Step 1: ç†è§£æ•´ä½“å‰§æƒ…
        print("\n[Step 1] ç†è§£å‰§æƒ…è„‰ç»œ...")
        plot_summary = self._understand_plot(scenes)
        print(f"   å‰§æƒ…æ¦‚è¦: {plot_summary[:100]}...")
        
        # Step 2: æ ‡è®°åœºæ™¯ç±»å‹
        print("\n[Step 2] åˆ†æåœºæ™¯ç±»å‹...")
        marked_scenes = self._mark_scenes(scenes)
        
        # Step 3: ç”Ÿæˆè§£è¯´
        print("\n[Step 3] ç”Ÿæˆè§£è¯´æ–‡æ¡ˆ...")
        final_scenes = self._generate_narrations(marked_scenes, plot_summary, style)
        
        # Step 4: ä¼˜åŒ–è¿è´¯æ€§
        print("\n[Step 4] ä¼˜åŒ–å‰§æƒ…è¿è´¯æ€§...")
        final_scenes = self._optimize_continuity(final_scenes)
        
        # ç»Ÿè®¡
        original_count = sum(1 for s in final_scenes if s.audio_mode == AudioMode.ORIGINAL)
        voiceover_count = sum(1 for s in final_scenes if s.audio_mode == AudioMode.VOICEOVER)
        skip_count = sum(1 for s in final_scenes if s.audio_mode == AudioMode.SKIP)
        
        total_duration = sum(s.duration for s in final_scenes if s.audio_mode != AudioMode.SKIP)
        
        print("\n" + "="*60)
        print("ğŸ“Š åˆ†æç»“æœ:")
        print(f"   ğŸ”Š åŸå£°åœºæ™¯: {original_count} ({original_count*100//(original_count+voiceover_count+1)}%)")
        print(f"   ğŸ™ï¸ è§£è¯´åœºæ™¯: {voiceover_count} ({voiceover_count*100//(original_count+voiceover_count+1)}%)")
        print(f"   ğŸ”‡ è·³è¿‡åœºæ™¯: {skip_count}")
        print(f"   â±ï¸ é¢„è®¡æ—¶é•¿: {total_duration:.0f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
        print("="*60)
        
        # ç”Ÿæˆå®Œæ•´è§£è¯´æ–‡æœ¬ï¼ˆåªåŒ…å«è§£è¯´åœºæ™¯ï¼‰
        full_narration = self._compile_narration_text(final_scenes)
        
        return final_scenes, full_narration
    
    def _understand_plot(self, scenes: List[Dict]) -> str:
        """ç†è§£æ•´ä½“å‰§æƒ…"""
        # æ”¶é›†æ‰€æœ‰å¯¹è¯
        all_dialogues = []
        for scene in scenes:
            dialogue = scene.get('dialogue', '').strip()
            if dialogue and len(dialogue) > 10:
                # è¿‡æ»¤æ•æ„Ÿè¯
                dialogue = self._filter_sensitive(dialogue)
                if dialogue:
                    all_dialogues.append(dialogue)
        
        if not all_dialogues:
            return "æ— æ³•è¯†åˆ«å‰§æƒ…å†…å®¹"
        
        # ç”¨AIæ€»ç»“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.llm_model:
            combined = "\n".join(all_dialogues[:50])  # å–å‰50æ®µ
            summary = self._ai_summarize(combined)
            if summary:
                return summary
        
        # å¤‡ç”¨ï¼šç®€å•æ‹¼æ¥
        return " ".join(all_dialogues[:10])[:500]
    
    def _mark_scenes(self, scenes: List[Dict]) -> List[SceneSegment]:
        """æ ‡è®°æ¯ä¸ªåœºæ™¯çš„ç±»å‹"""
        result = []
        
        for i, scene in enumerate(scenes):
            dialogue = scene.get('dialogue', '').strip()
            emotion = scene.get('emotion', 'neutral')
            importance = scene.get('importance', 0.5)
            
            # è¿‡æ»¤æ•æ„Ÿè¯
            dialogue = self._filter_sensitive(dialogue)
            
            # å†³å®šéŸ³é¢‘æ¨¡å¼
            audio_mode, reason = self._decide_audio_mode(
                dialogue, emotion, importance
            )
            
            segment = SceneSegment(
                scene_id=scene.get('scene_id', i + 1),  # ä¿®å¤ï¼šä»è¾“å…¥è¯»å–scene_id
                start_time=scene.get('start_time', 0),
                end_time=scene.get('end_time', 0),
                dialogue=dialogue,
                narration="",  # ç¨åç”Ÿæˆ
                audio_mode=audio_mode,
                importance=importance,
                emotion=emotion,
                reason=reason
            )
            
            result.append(segment)
        
        # æ‰“å°ç»Ÿè®¡
        orig = sum(1 for s in result if s.audio_mode == AudioMode.ORIGINAL)
        voice = sum(1 for s in result if s.audio_mode == AudioMode.VOICEOVER)
        skip = sum(1 for s in result if s.audio_mode == AudioMode.SKIP)
        print(f"   åŸå£°: {orig}, è§£è¯´: {voice}, è·³è¿‡: {skip}")
        
        return result
    
    def _decide_audio_mode(
        self, 
        dialogue: str, 
        emotion: str, 
        importance: float
    ) -> Tuple[AudioMode, str]:
        """
        å†³å®šåœºæ™¯çš„éŸ³é¢‘æ¨¡å¼
        
        ç”µè§†å‰§æ¨¡å¼ï¼šæ›´å€¾å‘äºè§£è¯´ï¼ˆè®²æ•…äº‹ï¼‰
        ç”µå½±æ¨¡å¼ï¼šæ›´å€¾å‘äºåŸå£°ï¼ˆä¿ç•™ç»å…¸ï¼‰
        """
        # å¼ºæƒ…æ„Ÿ â†’ åŸå£°ï¼ˆä¸¤ç§æ¨¡å¼éƒ½ä¿ç•™ï¼‰
        if emotion in ['angry', 'sad', 'excited', 'happy', 'fear']:
            return AudioMode.ORIGINAL, f"å¼ºæƒ…æ„Ÿåœºæ™¯({emotion})"
        
        # æ ¹æ®åª’ä½“ç±»å‹è°ƒæ•´é˜ˆå€¼
        if self.media_type == "tv":
            # ç”µè§†å‰§æ¨¡å¼ï¼šæ›´å¤šè§£è¯´
            original_threshold = 0.7  # åªæœ‰é«˜é‡è¦æ€§æ‰ç”¨åŸå£°
            voiceover_threshold = 0.25  # ä¸­ç­‰ä»¥ä¸Šéƒ½ç”¨è§£è¯´
            dialogue_threshold = 20  # è¾ƒé•¿å¯¹è¯æ‰ç”¨åŸå£°
        else:
            # ç”µå½±æ¨¡å¼ï¼šæ›´å¤šåŸå£°
            original_threshold = 0.5  # ä¸­ç­‰ä»¥ä¸Šç”¨åŸå£°
            voiceover_threshold = 0.35  # è¾ƒä½æ‰ç”¨è§£è¯´
            dialogue_threshold = 12  # çŸ­å¯¹è¯ä¹Ÿç”¨åŸå£°
        
        # æœ‰å¯¹è¯çš„åœºæ™¯
        if dialogue and len(dialogue) > dialogue_threshold:
            if importance >= original_threshold:
                return AudioMode.ORIGINAL, "é‡è¦å¯¹è¯"
            else:
                return AudioMode.VOICEOVER, "ç”¨è§£è¯´æ¦‚æ‹¬å¯¹è¯"
        
        # é«˜é‡è¦æ€§ â†’ åŸå£°
        if importance >= original_threshold:
            return AudioMode.ORIGINAL, "é«˜é‡è¦æ€§åœºæ™¯"
        
        # ä¸­ç­‰é‡è¦æ€§ â†’ è§£è¯´
        if importance >= voiceover_threshold:
            return AudioMode.VOICEOVER, "è¿‡æ¸¡åœºæ™¯,ç”¨è§£è¯´"
        
        # ä½é‡è¦æ€§ â†’ è·³è¿‡
        return AudioMode.SKIP, "ä½é‡è¦æ€§,è·³è¿‡"
    
    def _generate_narrations(
        self, 
        scenes: List[SceneSegment],
        plot_summary: str,
        style: str
    ) -> List[SceneSegment]:
        """ä¸ºè§£è¯´åœºæ™¯ç”Ÿæˆæ–‡æ¡ˆ"""
        
        for scene in scenes:
            if scene.audio_mode != AudioMode.VOICEOVER:
                continue
            
            # ç”Ÿæˆè§£è¯´
            narration = self._generate_single_narration(
                scene, plot_summary, style
            )
            
            # è´¨é‡æ£€æŸ¥
            if self._is_low_quality(narration):
                # ä½è´¨é‡ï¼Œæ”¹ä¸ºåŸå£°
                scene.audio_mode = AudioMode.ORIGINAL
                scene.reason = "è§£è¯´è´¨é‡ä¸ä½³,æ”¹ç”¨åŸå£°"
            else:
                scene.narration = narration
        
        return scenes
    
    def _generate_single_narration(
        self, 
        scene: SceneSegment,
        plot_summary: str,
        style: str
    ) -> str:
        """
        ç”Ÿæˆå•ä¸ªåœºæ™¯çš„è§£è¯´
        
        æ ¸å¿ƒï¼šåŸºäºå¯¹è¯å†…å®¹ç”Ÿæˆï¼Œä¸æ˜¯æ³›æ³›è€Œè°ˆ
        """
        dialogue = scene.dialogue
        
        if not dialogue:
            return ""
        
        # å°è¯•AIç”Ÿæˆ
        if self.llm_model:
            narration = self._ai_generate_narration(dialogue, style)
            if narration and not self._is_low_quality(narration):
                return narration
        
        # å¤‡ç”¨ï¼šåŸºäºå¯¹è¯å†…å®¹ç”Ÿæˆæ›´å¥½çš„è§£è¯´
        # å…³é”®ï¼šè¦æ¦‚æ‹¬ï¼Œä¸æ˜¯æˆªå–
        
        # æ ¹æ®æƒ…æ„Ÿç”Ÿæˆä¸åŒé£æ ¼çš„è§£è¯´
        if scene.emotion == 'angry':
            templates = [
                f"åŒæ–¹å‘ç”Ÿäº†æ¿€çƒˆçš„äº‰æ‰§",
                f"æ°”æ°›ä¸€ä¸‹å­ç´§å¼ èµ·æ¥",
                f"å†²çªåœ¨æ­¤åˆ»çˆ†å‘",
            ]
        elif scene.emotion == 'sad':
            templates = [
                f"æ°”æ°›å˜å¾—æ²‰é‡èµ·æ¥",
                f"æ‚²ä¼¤çš„æƒ…ç»ªè”“å»¶å¼€æ¥",
                f"è¿™ä¸€å¹•ä»¤äººåŠ¨å®¹",
            ]
        elif scene.emotion == 'happy':
            templates = [
                f"æ°”æ°›å˜å¾—è½»æ¾æ„‰å¿«",
                f"éš¾å¾—çš„æ¸©é¦¨æ—¶åˆ»",
                f"å¤§å®¶éƒ½éœ²å‡ºäº†ç¬‘å®¹",
            ]
        elif scene.emotion == 'fear':
            templates = [
                f"ç´§å¼ çš„æ°”æ°›è®©äººçª’æ¯",
                f"å±é™©æ­£åœ¨é€¼è¿‘",
                f"æ‰€æœ‰äººéƒ½å±ä½äº†å‘¼å¸",
            ]
        else:
            # neutral - æ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆ
            if len(dialogue) > 30:
                # æœ‰è¾ƒé•¿å¯¹è¯ï¼Œæå–å…³é”®ä¿¡æ¯
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­
                for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ']:
                    idx = dialogue.find(punct)
                    if idx > 5 and idx < 50:
                        return dialogue[:idx+1]
                return dialogue[:40] + "..."
            else:
                templates = [
                    f"æ•…äº‹ç»§ç»­å‘å±•",
                    f"æƒ…èŠ‚æ¨è¿›ä¸­",
                ]
        
        import random
        return random.choice(templates)
    
    def _ai_summarize(self, text: str) -> str:
        """ç”¨AIæ€»ç»“æ–‡æœ¬"""
        if not self.llm_model:
            return ""
        
        try:
            import ollama
            
            prompt = f"""è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹å¯¹è¯çš„ä¸»è¦å‰§æƒ…ï¼ˆä¸è¶…è¿‡100å­—ï¼‰ï¼š

{text[:2000]}

æ€»ç»“ï¼š"""
            
            response = ollama.chat(
                model=self.llm_model,
                messages=[{'role': 'user', 'content': prompt}],
                options={'num_predict': 150, 'temperature': 0.3}
            )
            
            result = response['message']['content'].strip()
            return self._filter_sensitive(result)
            
        except Exception as e:
            return ""
    
    def _ai_generate_narration(self, dialogue: str, style: str) -> str:
        """ç”¨AIç”Ÿæˆè§£è¯´ï¼ˆåŒºåˆ†ç”µè§†å‰§/ç”µå½±æ¨¡å¼ï¼‰"""
        if not self.llm_model:
            return ""
        
        try:
            import ollama
            
            # æ„å»ºä¸Šä¸‹æ–‡
            if self.media_type == "tv" and hasattr(self, 'episode_plot') and self.episode_plot:
                context = f"""æœ¬é›†å‰§æƒ…èƒŒæ™¯ï¼š{self.episode_plot[:200]}

å½“å‰åœºæ™¯å¯¹è¯ï¼š
{dialogue[:300]}"""
                task = f"ä¸ºè¿™ä¸ªç”µè§†å‰§ç‰‡æ®µç”Ÿæˆä¸€å¥{style}é£æ ¼çš„è§£è¯´ï¼ˆ20-40å­—ï¼‰ï¼Œè¦ç»“åˆæœ¬é›†å‰§æƒ…èƒŒæ™¯"
            else:
                context = f"å¯¹è¯å†…å®¹ï¼š{dialogue[:300]}"
                task = f"ä¸ºè¿™ä¸ªç‰‡æ®µç”Ÿæˆä¸€å¥{style}é£æ ¼çš„è§£è¯´ï¼ˆ15-30å­—ï¼‰"
            
            prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„è§†é¢‘è§£è¯´å‘˜ã€‚{task}ã€‚

{context}

è¦æ±‚ï¼š
1. æ¦‚æ‹¬å¯¹è¯å†…å®¹ï¼Œè®²è¿°æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…
2. è¯­è¨€è‡ªç„¶æµç•…ï¼ŒåƒçœŸäººè®²æ•…äº‹
3. ç¦æ­¢ä½¿ç”¨"ç´§å¼ çš„åœºé¢"ã€"ç´§å¼ çš„ä¸€å¹•"ã€"ç²¾å½©ç”»é¢"ç­‰ç©ºæ´æè¿°
4. ç¦æ­¢æ¶‰åŠä»»ä½•æ”¿æ²»äººç‰©æˆ–æ•æ„Ÿå†…å®¹
5. å¯ä»¥é€‚å½“åŠ å…¥è§’è‰²åå­—ï¼ˆå¦‚æœèƒ½ä»å¯¹è¯ä¸­è¯†åˆ«ï¼‰

ç›´æ¥è¾“å‡ºè§£è¯´å†…å®¹ï¼Œä¸è¦åŠ ä»»ä½•å‰ç¼€ï¼š"""
            
            response = ollama.chat(
                model=self.llm_model,
                messages=[{'role': 'user', 'content': prompt}],
                options={'num_predict': 100, 'temperature': 0.65}
            )
            
            result = response['message']['content'].strip()
            
            # æ¸…ç†æ ¼å¼
            result = result.replace('è§£è¯´ï¼š', '').replace('è§£è¯´:', '')
            result = result.replace('æ—ç™½ï¼š', '').replace('æ—ç™½:', '')
            result = result.strip('"\'""''')
            
            # ç§»é™¤å¯èƒ½çš„æ•°å­—åºå·
            import re
            result = re.sub(r'^[\d]+[\.ã€]\s*', '', result)
            
            return self._filter_sensitive(result)
            
        except Exception as e:
            return ""
    
    def _optimize_continuity(self, scenes: List[SceneSegment]) -> List[SceneSegment]:
        """ä¼˜åŒ–å‰§æƒ…è¿è´¯æ€§"""
        # è§„åˆ™1ï¼šä¸èƒ½è¿ç»­è¶…è¿‡Nä¸ªè§£è¯´åœºæ™¯ï¼ˆä¼šè®©è§‚ä¼—ç–²åŠ³ï¼‰
        max_consecutive = 7 if self.media_type == "tv" else 4  # ç”µè§†å‰§å…è®¸æ›´å¤šè¿ç»­è§£è¯´
        consecutive_voiceover = 0
        
        for scene in scenes:
            if scene.audio_mode == AudioMode.VOICEOVER:
                consecutive_voiceover += 1
                if consecutive_voiceover > max_consecutive and scene.dialogue:
                    # å¼ºåˆ¶æ”¹ä¸ºåŸå£°ï¼ˆæ’å…¥åŸå£°è®©è§‚ä¼—ä¼‘æ¯ï¼‰
                    scene.audio_mode = AudioMode.ORIGINAL
                    scene.reason = "é˜²æ­¢è¿ç»­è§£è¯´,æ’å…¥åŸå£°"
                    consecutive_voiceover = 0
            else:
                consecutive_voiceover = 0
        
        # è§„åˆ™2ï¼šç¡®ä¿æœ€ä½åŸå£°æ¯”ä¾‹
        orig_count = sum(1 for s in scenes if s.audio_mode == AudioMode.ORIGINAL)
        total = sum(1 for s in scenes if s.audio_mode != AudioMode.SKIP)
        
        if total > 0 and orig_count / total < self.min_original_ratio:
            # åŸå£°æ¯”ä¾‹å¤ªä½ï¼Œå°†éƒ¨åˆ†è§£è¯´æ”¹ä¸ºåŸå£°ï¼ˆé€‰é‡è¦æ€§é«˜çš„ï¼‰
            voiceover_scenes = [s for s in scenes if s.audio_mode == AudioMode.VOICEOVER]
            voiceover_scenes.sort(key=lambda x: x.importance, reverse=True)
            
            need_convert = int(total * self.min_original_ratio) - orig_count
            for i, scene in enumerate(voiceover_scenes):
                if i >= need_convert:
                    break
                if scene.dialogue:
                    scene.audio_mode = AudioMode.ORIGINAL
                    scene.reason = "å¢åŠ åŸå£°æ¯”ä¾‹"
        
        return scenes
    
    def _compile_narration_text(self, scenes: List[SceneSegment]) -> str:
        """ç¼–è¯‘å®Œæ•´è§£è¯´æ–‡æœ¬ï¼ˆä¾›TTSä½¿ç”¨ï¼‰"""
        narrations = []
        
        for scene in scenes:
            if scene.audio_mode == AudioMode.VOICEOVER and scene.narration:
                narrations.append(scene.narration)
        
        return "\n".join(narrations)
    
    def _filter_sensitive(self, text: str) -> str:
        """è¿‡æ»¤æ•æ„Ÿè¯"""
        if not text:
            return ""
        
        result = text
        for word in SENSITIVE_WORDS:
            if word in result:
                result = result.replace(word, "")
        
        return result.strip()
    
    def _is_low_quality(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯ä½è´¨é‡å†…å®¹"""
        if not text or len(text) < 5:
            return True
        
        for pattern in BAD_PATTERNS:
            if pattern in text:
                return True
        
        return False


def create_production_timeline(
    scenes: List[SceneSegment]
) -> List[Dict]:
    """
    åˆ›å»ºæœ€ç»ˆåˆ¶ä½œæ—¶é—´çº¿
    
    è¿”å›æ ¼å¼ï¼š
    [
        {
            'scene_id': 1,
            'source_start': 0.0,
            'source_end': 30.0,
            'output_start': 0.0,
            'output_end': 30.0,
            'audio_mode': 'original',  # or 'voiceover'
            'narration': '...',  # å¦‚æœæ˜¯è§£è¯´æ¨¡å¼
            'dialogue': '...',   # åŸå§‹å¯¹è¯
        },
        ...
    ]
    """
    timeline = []
    output_time = 0.0
    
    for scene in scenes:
        if scene.audio_mode == AudioMode.SKIP:
            continue
        
        item = {
            'scene_id': scene.scene_id,
            'source_start': scene.start_time,
            'source_end': scene.end_time,
            'output_start': output_time,
            'output_end': output_time + scene.duration,
            'audio_mode': scene.audio_mode.value,
            'narration': scene.narration,
            'dialogue': scene.dialogue,
            'emotion': scene.emotion,
            'reason': scene.reason,
        }
        
        timeline.append(item)
        output_time += scene.duration
    
    return timeline


# æµ‹è¯•
if __name__ == "__main__":
    engine = NarrationEngine(use_ai=True)
    
    # æ¨¡æ‹Ÿåœºæ™¯
    test_scenes = [
        {'start_time': 0, 'end_time': 30, 'dialogue': 'ä½ æ˜¯è°ï¼Ÿä¸ºä»€ä¹ˆè¦æ¥è¿™é‡Œï¼Ÿ', 'emotion': 'angry', 'importance': 0.9},
        {'start_time': 30, 'end_time': 60, 'dialogue': 'æˆ‘æœ‰è¯è¦å‘Šè¯‰ä½ ', 'emotion': 'neutral', 'importance': 0.5},
        {'start_time': 60, 'end_time': 90, 'dialogue': '', 'emotion': 'neutral', 'importance': 0.2},
        {'start_time': 90, 'end_time': 120, 'dialogue': 'è¿™ä»¶äº‹æƒ…éå¸¸é‡è¦ï¼Œä½ å¿…é¡»çŸ¥é“çœŸç›¸', 'emotion': 'sad', 'importance': 0.8},
    ]
    
    segments, narration = engine.analyze_and_generate(test_scenes, "æµ‹è¯•å‰§", "å¹½é»˜")
    
    print("\næœ€ç»ˆæ—¶é—´çº¿:")
    for seg in segments:
        mode = "ğŸ”ŠåŸå£°" if seg.audio_mode == AudioMode.ORIGINAL else ("ğŸ™ï¸è§£è¯´" if seg.audio_mode == AudioMode.VOICEOVER else "ğŸ”‡è·³è¿‡")
        print(f"  {seg.start_time:.0f}s-{seg.end_time:.0f}s: {mode} - {seg.reason}")

