# core/semantic_matcher.py - è¯­ä¹‰åŒ¹é…ç´ æé€‰æ‹©å™¨
"""
SmartVideoClipper v3.0 - è¯­ä¹‰åŒ¹é…ç´ æé€‰æ‹©å™¨

æ ¸å¿ƒåŠŸèƒ½ï¼šæ ¹æ®è§£è¯´å‰§æœ¬ä¸­çš„åœºæ™¯æè¿°ï¼Œç²¾ç¡®åŒ¹é…åŸè§†é¢‘ç”»é¢

æŠ€æœ¯æ–¹æ¡ˆï¼š
1. Chinese-CLIP: è®¡ç®—æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦
2. å¯¹è¯åŒ¹é…ï¼šæ ¹æ®å­—å¹•å†…å®¹åŒ¹é…åœºæ™¯
3. æ—¶é—´çº¦æŸï¼šåœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœç´¢

è¿™æ˜¯å®ç°"è§£è¯´-ç”»é¢åŒæ­¥"çš„å…³é”®ï¼
"""

import cv2
import numpy as np
import torch
from typing import Dict, List, Tuple, Optional
import os


class SemanticMatcher:
    """
    è¯­ä¹‰åŒ¹é…å™¨
    
    è¾“å…¥ï¼šè§£è¯´å‰§æœ¬ + è§†é¢‘å¸§ + å­—å¹•
    è¾“å‡ºï¼šæ¯æ®µè§£è¯´å¯¹åº”çš„ç²¾ç¡®è§†é¢‘ç‰‡æ®µ
    """
    
    def __init__(self):
        self.clip_model = None
        self.clip_preprocess = None
        self.tokenizer = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self._load_clip()
    
    def _load_clip(self):
        """åŠ è½½Chinese-CLIPæ¨¡å‹"""
        try:
            import cn_clip.clip as clip
            from cn_clip.clip import load_from_name
            
            print("[CLIP] åŠ è½½ Chinese-CLIP æ¨¡å‹...")
            self.clip_model, self.clip_preprocess = load_from_name(
                "ViT-B-16",
                device=self.device,
                download_root='./models'
            )
            self.clip_model.eval()
            self.tokenizer = clip.tokenize
            print(f"[CLIP] æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.device}")
            
        except Exception as e:
            print(f"[WARNING] CLIPåŠ è½½å¤±è´¥: {e}")
            self.clip_model = None
    
    def match_segments(
        self,
        video_path: str,
        script_segments: List[Dict],
        transcript_segments: List[Dict],
        scenes: List[Dict]
    ) -> List[Dict]:
        """
        ä¸ºæ¯æ®µè§£è¯´åŒ¹é…æœ€ä½³è§†é¢‘ç´ æ
        
        å‚æ•°ï¼š
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            script_segments: è§£è¯´å‰§æœ¬æ®µè½
            transcript_segments: å­—å¹•ç‰‡æ®µï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
            scenes: åœºæ™¯åˆ—è¡¨ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        
        è¿”å›ï¼š
            æ›´æ–°åçš„script_segmentsï¼Œæ¯æ®µåŒ…å«matched_clips
        """
        print("\n" + "="*60)
        print("ğŸ” è¯­ä¹‰åŒ¹é…ç´ æé€‰æ‹©å™¨ v3.0")
        print("="*60)
        
        if not os.path.exists(video_path):
            print(f"[ERROR] è§†é¢‘ä¸å­˜åœ¨: {video_path}")
            return script_segments
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        total_duration = total_frames / fps if fps > 0 else 0
        
        print(f"   è§†é¢‘æ—¶é•¿: {total_duration:.0f}ç§’")
        print(f"   è§£è¯´æ®µè½: {len(script_segments)}æ®µ")
        
        # ä¸ºæ¯æ®µåŒ¹é…ç´ æ
        for i, seg in enumerate(script_segments):
            print(f"\n[{i+1}/{len(script_segments)}] åŒ¹é…: {seg.get('phase', 'æœªçŸ¥æ®µè½')}")
            
            # è·å–æœç´¢æ—¶é—´èŒƒå›´
            time_range = seg.get('source_time_range', [0, total_duration])
            start_time, end_time = time_range
            
            # è·å–åœºæ™¯æè¿°
            scene_desc = seg.get('scene_description', '')
            narration = seg.get('narration_text', '')
            
            # å¤šç­–ç•¥åŒ¹é…
            matched_clips = []
            
            # ç­–ç•¥1ï¼šåŸºäºCLIPçš„è§†è§‰åŒ¹é…
            if self.clip_model and scene_desc:
                clip_matches = self._match_by_clip(
                    cap, fps, scene_desc, start_time, end_time
                )
                matched_clips.extend(clip_matches)
                print(f"   [CLIP] æ‰¾åˆ° {len(clip_matches)} ä¸ªåŒ¹é…")
            
            # ç­–ç•¥2ï¼šåŸºäºå¯¹è¯çš„åŒ¹é…
            dialogue_matches = self._match_by_dialogue(
                transcript_segments, narration, start_time, end_time
            )
            matched_clips.extend(dialogue_matches)
            print(f"   [å¯¹è¯] æ‰¾åˆ° {len(dialogue_matches)} ä¸ªåŒ¹é…")
            
            # ç­–ç•¥3ï¼šåŸºäºåœºæ™¯åˆ‡æ¢çš„åŒ¹é…
            scene_matches = self._match_by_scenes(
                scenes, start_time, end_time, seg.get('duration', 30)
            )
            matched_clips.extend(scene_matches)
            print(f"   [åœºæ™¯] æ‰¾åˆ° {len(scene_matches)} ä¸ªåŒ¹é…")
            
            # åˆå¹¶å’Œå»é‡
            final_clips = self._merge_clips(matched_clips, seg.get('duration', 30))
            seg['matched_clips'] = final_clips
            
            print(f"   âœ“ æœ€ç»ˆé€‰å– {len(final_clips)} ä¸ªç‰‡æ®µ")
        
        cap.release()
        
        print("\n" + "="*60)
        print("âœ… ç´ æåŒ¹é…å®Œæˆï¼")
        print("="*60)
        
        return script_segments
    
    def _match_by_clip(
        self,
        cap: cv2.VideoCapture,
        fps: float,
        scene_description: str,
        start_time: float,
        end_time: float,
        sample_interval: float = 3.0
    ) -> List[Dict]:
        """ä½¿ç”¨CLIPè¿›è¡Œè§†è§‰-æ–‡æœ¬åŒ¹é…"""
        
        if not self.clip_model:
            return []
        
        matches = []
        
        try:
            import cn_clip.clip as clip
            from PIL import Image
            
            # ç¼–ç æ–‡æœ¬
            text = self.tokenizer([scene_description]).to(self.device)
            with torch.no_grad():
                text_features = self.clip_model.encode_text(text)
                text_features /= text_features.norm(dim=-1, keepdim=True)
            
            # é‡‡æ ·å¸§å¹¶è®¡ç®—ç›¸ä¼¼åº¦
            candidates = []
            
            for t in np.arange(start_time, end_time, sample_interval):
                frame_num = int(t * fps)
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
                ret, frame = cap.read()
                
                if not ret or frame is None:
                    continue
                
                # è½¬æ¢ä¸ºPILå›¾åƒ
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                
                # é¢„å¤„ç†å¹¶ç¼–ç 
                image_input = self.clip_preprocess(pil_image).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    image_features = self.clip_model.encode_image(image_input)
                    image_features /= image_features.norm(dim=-1, keepdim=True)
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = (text_features @ image_features.T).item()
                
                candidates.append({
                    'time': t,
                    'similarity': similarity,
                    'method': 'clip'
                })
            
            # é€‰æ‹©topåŒ¹é…
            candidates.sort(key=lambda x: x['similarity'], reverse=True)
            
            for cand in candidates[:3]:  # æœ€å¤š3ä¸ª
                if cand['similarity'] > 0.2:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    matches.append({
                        'start': cand['time'],
                        'end': cand['time'] + 5,
                        'score': cand['similarity'],
                        'method': 'clip'
                    })
            
        except Exception as e:
            print(f"   [WARNING] CLIPåŒ¹é…å¤±è´¥: {e}")
        
        return matches
    
    def _match_by_dialogue(
        self,
        transcript_segments: List[Dict],
        narration_text: str,
        start_time: float,
        end_time: float
    ) -> List[Dict]:
        """åŸºäºå¯¹è¯å†…å®¹åŒ¹é…"""
        
        matches = []
        
        # æå–è§£è¯´ä¸­çš„å…³é”®è¯
        keywords = self._extract_keywords(narration_text)
        
        for seg in transcript_segments:
            seg_start = seg.get('start', 0)
            seg_end = seg.get('end', seg_start + 3)
            seg_text = seg.get('text', '')
            
            # æ£€æŸ¥æ—¶é—´èŒƒå›´
            if seg_start < start_time or seg_end > end_time:
                continue
            
            # è®¡ç®—åŒ¹é…åº¦
            match_score = 0
            for kw in keywords:
                if kw in seg_text:
                    match_score += 1
            
            if match_score > 0:
                matches.append({
                    'start': seg_start,
                    'end': seg_end,
                    'score': match_score / max(len(keywords), 1),
                    'method': 'dialogue',
                    'matched_text': seg_text
                })
        
        # æŒ‰åˆ†æ•°æ’åº
        matches.sort(key=lambda x: x['score'], reverse=True)
        
        return matches[:5]  # æœ€å¤š5ä¸ª
    
    def _match_by_scenes(
        self,
        scenes: List[Dict],
        start_time: float,
        end_time: float,
        target_duration: float
    ) -> List[Dict]:
        """åŸºäºåœºæ™¯åˆ‡æ¢åŒ¹é…"""
        
        matches = []
        
        # æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„åœºæ™¯
        for scene in scenes:
            scene_start = scene.get('start', scene.get('start_time', 0))
            scene_end = scene.get('end', scene.get('end_time', scene_start + 5))
            
            # æ—¶é—´è½¬æ¢ï¼ˆå¦‚æœæ˜¯å¸§æ•°ï¼‰
            if scene_start > 1000:  # å¯èƒ½æ˜¯å¸§æ•°
                scene_start = scene_start / 25  # å‡è®¾25fps
                scene_end = scene_end / 25
            
            if scene_start >= start_time and scene_end <= end_time:
                duration = scene_end - scene_start
                
                matches.append({
                    'start': scene_start,
                    'end': scene_end,
                    'score': min(duration / target_duration, 1.0),
                    'method': 'scene',
                    'duration': duration
                })
        
        # æŒ‰æ—¶é—´æ’åº
        matches.sort(key=lambda x: x['start'])
        
        return matches
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        import re
        
        # æƒ…æ„Ÿè¯
        emotion_words = ['æ„¤æ€’', 'æ‚²ä¼¤', 'æƒŠè®¶', 'ææƒ§', 'ç´§å¼ ', 'æ¸©æƒ…', 'å†²çª',
                        'å“­', 'ç¬‘', 'æ€', 'æ­»', 'çˆ±', 'æ¨', 'è·‘', 'è¿½']
        
        # åŠ¨ä½œè¯
        action_words = ['æ‰“', 'è·‘', 'è·³', 'å¼€æª', 'å¼€è½¦', 'æ‹¥æŠ±', 'äº²å»', 
                       'å¯¹è¯', 'äº‰åµ', 'é€ƒè·‘', 'è¿½é€', 'çˆ†ç‚¸']
        
        # åœºæ™¯è¯
        scene_words = ['è¡—é“', 'æˆ¿é—´', 'è­¦å¯Ÿå±€', 'åŒ»é™¢', 'å­¦æ ¡', 'é¤å…',
                      'å¤œæ™š', 'ç™½å¤©', 'é›¨', 'é›ª']
        
        keywords = []
        
        for word in emotion_words + action_words + scene_words:
            if word in text:
                keywords.append(word)
        
        # æå–äººåï¼ˆä¸­æ–‡åé€šå¸¸2-3ä¸ªå­—ï¼‰
        name_pattern = r'[\u4e00-\u9fff]{2,3}(?=è¯´|é“|é—®|ç­”|æƒ³|çœ‹|èµ°|è·‘)'
        names = re.findall(name_pattern, text)
        keywords.extend(names[:3])
        
        return keywords
    
    def _merge_clips(
        self,
        clips: List[Dict],
        target_duration: float
    ) -> List[Dict]:
        """åˆå¹¶å’Œç­›é€‰ç‰‡æ®µ"""
        
        if not clips:
            return []
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        clips.sort(key=lambda x: x['start'])
        
        # åˆå¹¶é‡å ç‰‡æ®µ
        merged = []
        current = clips[0].copy()
        
        for clip in clips[1:]:
            if clip['start'] <= current['end'] + 1:
                # åˆå¹¶
                current['end'] = max(current['end'], clip['end'])
                current['score'] = max(current['score'], clip['score'])
            else:
                merged.append(current)
                current = clip.copy()
        merged.append(current)
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€ä½³ç‰‡æ®µ
        merged.sort(key=lambda x: x['score'], reverse=True)
        
        # æ§åˆ¶æ€»æ—¶é•¿
        selected = []
        total_duration = 0
        
        for clip in merged:
            clip_duration = clip['end'] - clip['start']
            if total_duration + clip_duration <= target_duration * 1.5:
                selected.append(clip)
                total_duration += clip_duration
        
        # æŒ‰æ—¶é—´é¡ºåºè¿”å›
        selected.sort(key=lambda x: x['start'])
        
        return selected


# æ™ºèƒ½å‰ªè¾‘å™¨
class SmartClipper:
    """
    æ™ºèƒ½å‰ªè¾‘å™¨
    
    æ ¹æ®åŒ¹é…ç»“æœæ‰§è¡Œç²¾ç¡®å‰ªè¾‘
    """
    
    def __init__(self):
        pass
    
    def create_timeline(
        self,
        script_segments: List[Dict],
        video_duration: float
    ) -> List[Dict]:
        """
        åˆ›å»ºå‰ªè¾‘æ—¶é—´çº¿
        
        è¿”å›ï¼š
        [
            {
                'clip_id': 1,
                'source_start': 100.0,
                'source_end': 120.0,
                'narration_start': 0.0,
                'narration_end': 20.0,
                'keep_original': False,
            },
            ...
        ]
        """
        timeline = []
        narration_cursor = 0.0
        clip_id = 0
        
        for seg in script_segments:
            matched_clips = seg.get('matched_clips', [])
            narration_duration = seg.get('duration', 30)
            keep_original = seg.get('keep_original_audio', False)
            
            if not matched_clips:
                # ä½¿ç”¨å»ºè®®çš„æ—¶é—´èŒƒå›´
                time_range = seg.get('source_time_range', [0, 30])
                matched_clips = [{
                    'start': time_range[0],
                    'end': min(time_range[0] + narration_duration, time_range[1])
                }]
            
            # å°†ç´ æåˆ†é…åˆ°æ—¶é—´çº¿
            for clip in matched_clips:
                clip_id += 1
                clip_duration = clip['end'] - clip['start']
                
                timeline.append({
                    'clip_id': clip_id,
                    'segment_id': seg.get('segment_id'),
                    'phase': seg.get('phase', ''),
                    'source_start': clip['start'],
                    'source_end': clip['end'],
                    'narration_start': narration_cursor,
                    'narration_end': narration_cursor + clip_duration,
                    'keep_original': keep_original,
                    'narration_text': seg.get('narration_text', '')[:50] + '...'
                })
                
                narration_cursor += clip_duration
        
        return timeline
    
    def print_timeline(self, timeline: List[Dict]):
        """æ‰“å°æ—¶é—´çº¿"""
        print("\n" + "="*70)
        print("ğŸ“‹ å‰ªè¾‘æ—¶é—´çº¿")
        print("="*70)
        print(f"{'#':<4} {'é˜¶æ®µ':<12} {'æºè§†é¢‘':<20} {'è§£è¯´æ—¶é—´':<20} {'åŸå£°':<6}")
        print("-"*70)
        
        for item in timeline:
            source = f"{item['source_start']:.1f}s - {item['source_end']:.1f}s"
            narr = f"{item['narration_start']:.1f}s - {item['narration_end']:.1f}s"
            orig = "âœ“" if item['keep_original'] else ""
            
            print(f"{item['clip_id']:<4} {item['phase']:<12} {source:<20} {narr:<20} {orig:<6}")
        
        print("="*70)


# æµ‹è¯•
if __name__ == "__main__":
    matcher = SemanticMatcher()
    clipper = SmartClipper()
    
    # æ¨¡æ‹Ÿæ•°æ®
    test_script = [
        {
            'segment_id': 1,
            'phase': 'å¼€åœºç™½',
            'scene_description': 'ä¸€ä¸ªç”·äººç«™åœ¨è¡—å¤´',
            'narration_text': 'ä»Šå¤©è¦ç»™å¤§å®¶ä»‹ç»çš„æ˜¯ä¸€ä¸ªå…³äºæˆé•¿çš„æ•…äº‹',
            'source_time_range': [0, 60],
            'duration': 20,
        },
        {
            'segment_id': 2,
            'phase': 'é«˜æ½®',
            'scene_description': 'æ¿€çƒˆçš„å†²çªåœºé¢',
            'narration_text': 'æ­¤åˆ»ï¼Œå‘½è¿çš„é½¿è½®å¼€å§‹è½¬åŠ¨',
            'source_time_range': [1000, 1200],
            'duration': 30,
            'keep_original_audio': True,
        },
    ]
    
    test_script[0]['matched_clips'] = [
        {'start': 10, 'end': 25, 'score': 0.8},
        {'start': 40, 'end': 55, 'score': 0.6},
    ]
    test_script[1]['matched_clips'] = [
        {'start': 1050, 'end': 1080, 'score': 0.9},
    ]
    
    timeline = clipper.create_timeline(test_script, 2400)
    clipper.print_timeline(timeline)

