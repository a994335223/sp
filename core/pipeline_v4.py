# core/pipeline_v4.py - ç¬¬å››ä»£å¤„ç†ç®¡çº¿
"""
SmartVideoClipper v4.0 - åŸºäºè§†é¢‘å†…å®¹çš„è§£è¯´ç”Ÿæˆ

æ ¸å¿ƒæ”¹è¿›ï¼š
1. çœ‹ç”»é¢å†™è§£è¯´ï¼ˆä¸æ˜¯å†™è§£è¯´æ‰¾ç”»é¢ï¼‰
2. è§£è¯´å’ŒåŸå£°äºŒé€‰ä¸€ï¼ˆä¸æ··åˆï¼‰
3. ç”»é¢-è§£è¯´ç²¾ç¡®å¯¹é½
4. TMDB API è·å–è¯¦ç»†å‰§æƒ…

çœŸæ­£åšåˆ°ï¼šå…¨çƒç¬¬ä¸€çš„æ™ºèƒ½è§†é¢‘è§£è¯´
"""

import asyncio
import os
import sys
from pathlib import Path
from typing import Optional, Callable, List, Dict
from datetime import datetime
import json

# è®¾ç½®ç¯å¢ƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_OFFLINE"] = "0"

# å¯¼å…¥æ¨¡å—
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "core"))

from intro_outro_detect import auto_trim_intro_outro
from scene_detect import detect_scenes
from transcribe import transcribe_video
from tts_synthesis import TTSEngine
from smart_cut import extract_clips, concat_clips
from compose_video import compose_v4, add_subtitles, convert_to_douyin
from plot_fetcher import PlotFetcher, get_plot_info, parse_episode_from_filename
from video_content_analyzer import VideoContentAnalyzer, create_scene_based_timeline


# å¤„ç†æ­¥éª¤
PROCESS_STEPS_V4 = [
    (0, "é¢„å¤„ç†", "æ£€æµ‹å¹¶å»é™¤ç‰‡å¤´ç‰‡å°¾"),
    (1, "å‰§æƒ…è·å–", "TMDB APIè·å–è¯¦ç»†å‰§æƒ…"),
    (2, "è¯­éŸ³è¯†åˆ«", "è¯†åˆ«è§†é¢‘å¯¹ç™½"),
    (3, "åœºæ™¯åˆ†æ", "åˆ†ææ¯ä¸ªåœºæ™¯çš„å†…å®¹"),
    (4, "ç”Ÿæˆè§£è¯´", "åŸºäºç”»é¢å†…å®¹å†™è§£è¯´"),
    (5, "ç´ æå‰ªè¾‘", "ç²¾ç¡®å‰ªè¾‘è§†é¢‘ç´ æ"),
    (6, "è¯­éŸ³åˆæˆ", "ç”Ÿæˆè§£è¯´é…éŸ³"),
    (7, "åˆæˆè¾“å‡º", "è§£è¯´/åŸå£°åˆ†ç¦»åˆæˆ"),
]
TOTAL_STEPS_V4 = len(PROCESS_STEPS_V4)


class VideoPipelineV4:
    """
    ç¬¬å››ä»£è§†é¢‘å¤„ç†ç®¡çº¿
    
    æ ¸å¿ƒç†å¿µï¼šçœ‹ç”»é¢å†™è§£è¯´
    """
    
    def __init__(self):
        self.content_analyzer = VideoContentAnalyzer()
        self.tts_engine = TTSEngine()
    
    async def process(
        self,
        input_video: str,
        movie_name: str = None,
        output_name: str = "è§£è¯´è§†é¢‘",
        style: str = "å¹½é»˜",
        target_duration: int = 600,
        progress_callback: Optional[Callable] = None,
        tmdb_api_key: str = None
    ) -> dict:
        """
        æ‰§è¡Œå®Œæ•´å¤„ç†æµç¨‹
        """
        start_time = datetime.now()
        
        # åˆ›å»ºå·¥ä½œç›®å½•
        work_dir = Path(f"workspace_{output_name}_v4")
        work_dir.mkdir(exist_ok=True)
        
        def report_progress(step: int, detail: str):
            if progress_callback:
                step_name = PROCESS_STEPS_V4[step][1] if step < len(PROCESS_STEPS_V4) else "å®Œæˆ"
                progress_callback(step, TOTAL_STEPS_V4, step_name, detail)
            print(f"\n[Step {step}] {detail}")
        
        # æ‰“å°å¤´éƒ¨
        self._print_header(input_video, movie_name, style, target_duration)
        
        try:
            # ========== Step 0: é¢„å¤„ç† ==========
            report_progress(0, "æ­£åœ¨æ£€æµ‹ç‰‡å¤´ç‰‡å°¾...")
            
            trimmed_path = str(work_dir / "trimmed_video.mp4")
            processed_video, intro_offset, outro_time = auto_trim_intro_outro(
                input_video, trimmed_path, skip_if_short=300
            )
            if processed_video != input_video:
                print(f"   âœ“ å·²å»é™¤ç‰‡å¤´: {intro_offset:.1f}ç§’")
            
            # ========== Step 1: å‰§æƒ…è·å– ==========
            report_progress(1, f"æ­£åœ¨è·å–ã€Š{movie_name or 'æœªçŸ¥'}ã€‹çš„å‰§æƒ…...")
            
            # ä½¿ç”¨ TMDB API
            api_key = tmdb_api_key or os.environ.get("TMDB_API_KEY", "")
            season, episode = parse_episode_from_filename(input_video)
            
            plot_info = {}
            if api_key:
                fetcher = PlotFetcher(api_key)
                plot_info = fetcher.fetch(
                    title=movie_name or "æœªçŸ¥",
                    media_type="auto",
                    season=season,
                    episode=episode
                )
                fetcher.close()
                
                if plot_info.get('overview'):
                    print(f"   âœ“ TMDBè·å–æˆåŠŸï¼š{len(plot_info['overview'])}å­—")
            else:
                print("   [INFO] æœªé…ç½®TMDB APIï¼Œè·³è¿‡")
            
            # ========== Step 2: è¯­éŸ³è¯†åˆ« ==========
            report_progress(2, "æ­£åœ¨è¯†åˆ«è§†é¢‘å¯¹ç™½...")
            
            subtitle_path = str(work_dir / "subtitles.srt")
            segments, transcript = transcribe_video(
                processed_video,
                output_srt=subtitle_path
            )
            print(f"   âœ“ è¯†åˆ«åˆ° {len(segments)} æ®µå¯¹ç™½")
            
            # ========== Step 3: åœºæ™¯åˆ†æ ==========
            report_progress(3, "æ­£åœ¨åˆ†ææ¯ä¸ªåœºæ™¯çš„å†…å®¹...")
            
            # æ£€æµ‹åœºæ™¯
            scenes, _ = detect_scenes(processed_video, str(work_dir))
            print(f"   æ£€æµ‹åˆ° {len(scenes)} ä¸ªåœºæ™¯")
            
            # åˆ†æåœºæ™¯å†…å®¹
            analyzed_scenes = self.content_analyzer.analyze_video(
                video_path=processed_video,
                scenes=scenes,
                transcript_segments=segments
            )
            
            # ä¿å­˜åˆ†æç»“æœ
            with open(work_dir / "scene_analysis.json", 'w', encoding='utf-8') as f:
                # æ¸…ç†ä¸å¯åºåˆ—åŒ–çš„å†…å®¹
                clean_scenes = []
                for s in analyzed_scenes:
                    clean_scene = {k: v for k, v in s.items() if isinstance(v, (str, int, float, bool, list, dict, type(None)))}
                    clean_scenes.append(clean_scene)
                json.dump(clean_scenes, f, ensure_ascii=False, indent=2)
            
            # ========== Step 4: ç”Ÿæˆè§£è¯´ ==========
            report_progress(4, f"æ­£åœ¨ä¸ºæ¯ä¸ªåœºæ™¯ç”Ÿæˆ{style}é£æ ¼è§£è¯´...")
            
            # åŸºäºåœºæ™¯å†…å®¹ç”Ÿæˆè§£è¯´
            narrated_scenes = self.content_analyzer.generate_scene_narrations(
                analyzed_scenes=analyzed_scenes,
                target_duration=target_duration,
                style=style
            )
            
            # å¦‚æœæœ‰å‰§æƒ…ä¿¡æ¯ï¼Œç”¨AIå¢å¼ºè§£è¯´
            if plot_info.get('overview'):
                narrated_scenes = await self._enhance_narrations_with_plot(
                    narrated_scenes, plot_info, style
                )
            
            # ä¿å­˜è§£è¯´å‰§æœ¬
            script_text = self._format_scene_script(narrated_scenes)
            script_path = work_dir / "è§£è¯´å‰§æœ¬_v4.txt"
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(script_text)
            print(f"   âœ“ å‰§æœ¬å·²ä¿å­˜: {script_path}")
            
            # ========== Step 5: ç´ æå‰ªè¾‘ ==========
            report_progress(5, "æ­£åœ¨ç²¾ç¡®å‰ªè¾‘è§†é¢‘ç´ æ...")
            
            # åˆ›å»ºæ—¶é—´çº¿
            timeline = create_scene_based_timeline(narrated_scenes, target_duration)
            
            # æ‰“å°æ—¶é—´çº¿
            self._print_timeline(timeline)
            
            # æå–ç‰‡æ®µ
            clips_to_extract = [
                {'start': item['source_start'], 'end': item['source_end']}
                for item in timeline
            ]
            
            clips_dir = work_dir / "clips"
            clips_dir.mkdir(exist_ok=True)
            clip_files = extract_clips(processed_video, clips_to_extract, str(clips_dir))
            print(f"   âœ“ æå–äº† {len(clip_files)} ä¸ªç‰‡æ®µ")
            
            # æ‹¼æ¥
            concat_path = str(work_dir / "å‰ªè¾‘å.mp4")
            concat_clips(clip_files, concat_path)
            
            # ========== Step 6: è¯­éŸ³åˆæˆ ==========
            report_progress(6, "æ­£åœ¨ç”Ÿæˆè§£è¯´é…éŸ³...")
            
            # åªä¸ºéœ€è¦è§£è¯´çš„åœºæ™¯ç”ŸæˆTTS
            voiceover_text = '\n'.join([
                item.get('narration_text', '')
                for item in timeline
                if item.get('audio_mode') == 'voiceover' and item.get('narration_text')
            ])
            
            narration_path = str(work_dir / "narration.wav")
            if voiceover_text.strip():
                await self.tts_engine.synthesize(voiceover_text, narration_path)
                print(f"   âœ“ é…éŸ³å·²ç”Ÿæˆ: {narration_path}")
            else:
                # ç”Ÿæˆé™éŸ³éŸ³é¢‘
                print("   [INFO] æ— éœ€è§£è¯´ï¼Œç”Ÿæˆé™éŸ³éŸ³é¢‘")
                self._generate_silence(narration_path, 1.0)
            
            # ========== Step 7: åˆæˆè¾“å‡º ==========
            report_progress(7, "æ­£åœ¨åˆæˆæœ€ç»ˆè§†é¢‘ï¼ˆè§£è¯´/åŸå£°åˆ†ç¦»ï¼‰...")
            
            final_path = str(work_dir / f"{output_name}.mp4")
            compose_v4(
                video_clips=clip_files,
                narration_path=narration_path,
                output_path=final_path,
                timeline=timeline
            )
            
            # æ·»åŠ å­—å¹•
            final_with_sub = str(work_dir / f"{output_name}_sub.mp4")
            add_subtitles(final_path, subtitle_path, final_with_sub)
            
            # è½¬æŠ–éŸ³æ ¼å¼
            douyin_path = str(work_dir / f"{output_name}_æŠ–éŸ³.mp4")
            convert_to_douyin(final_with_sub, douyin_path)
            
            # å®Œæˆ
            elapsed = (datetime.now() - start_time).seconds
            self._print_footer(final_path, elapsed)
            
            return {
                'video_path': final_path,
                'douyin_path': douyin_path,
                'script_path': str(script_path),
                'subtitle_path': subtitle_path,
                'work_dir': str(work_dir),
                'timeline': timeline,
                'analyzed_scenes': len(analyzed_scenes),
            }
            
        except Exception as e:
            print(f"\n[ERROR] å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    async def _enhance_narrations_with_plot(
        self,
        scenes: List[Dict],
        plot_info: Dict,
        style: str
    ) -> List[Dict]:
        """ç”¨å‰§æƒ…ä¿¡æ¯å¢å¼ºè§£è¯´"""
        try:
            import ollama
            
            # è·å–å¯ç”¨æ¨¡å‹
            models = ollama.list()
            model = None
            if hasattr(models, 'models'):
                for m in models.models:
                    model = getattr(m, 'name', None)
                    if model:
                        break
            
            if not model:
                return scenes
            
            plot_summary = plot_info.get('overview', '')[:500]
            characters = plot_info.get('cast', [])[:5]
            char_names = [c.get('character', c.get('name', '')) for c in characters]
            
            print(f"   [AI] ä½¿ç”¨ {model} å¢å¼ºè§£è¯´...")
            
            for scene in scenes:
                if scene.get('narration_type') == 'voiceover' and scene.get('narration'):
                    original_narration = scene['narration']
                    
                    prompt = f"""ä½ æ˜¯å½±è§†è§£è¯´åšä¸»ï¼Œé£æ ¼{style}ã€‚

å‰§æƒ…èƒŒæ™¯ï¼š{plot_summary}
ä¸»è¦äººç‰©ï¼š{', '.join(char_names)}

å½“å‰ç”»é¢ï¼š{scene.get('visual_content', '')}
å½“å‰å¯¹è¯ï¼š{scene.get('dialogue', '')[:100]}

åŸæœ‰è§£è¯´ï¼š{original_narration}

è¯·æ”¹å†™è¿™æ®µè§£è¯´ï¼Œä½¿å…¶ï¼š
1. æ›´åŠ è´´åˆç”»é¢å†…å®¹
2. èå…¥å‰§æƒ…èƒŒæ™¯
3. è¯­è¨€ç”ŸåŠ¨æœ‰è¶£
4. æ§åˆ¶åœ¨50å­—ä»¥å†…

ç›´æ¥è¾“å‡ºæ”¹å†™åçš„è§£è¯´ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ï¼š"""

                    try:
                        response = ollama.chat(
                            model=model,
                            messages=[{'role': 'user', 'content': prompt}],
                            options={'temperature': 0.7, 'num_predict': 200}
                        )
                        
                        enhanced = response['message']['content'].strip()
                        if enhanced and len(enhanced) < 150:
                            scene['narration'] = enhanced
                    except:
                        pass
            
            return scenes
            
        except Exception as e:
            print(f"   [WARNING] AIå¢å¼ºå¤±è´¥: {e}")
            return scenes
    
    def _format_scene_script(self, scenes: List[Dict]) -> str:
        """æ ¼å¼åŒ–åœºæ™¯å‰§æœ¬"""
        lines = []
        lines.append("=" * 50)
        lines.append("SmartVideoClipper v4.0 - åœºæ™¯è§£è¯´å‰§æœ¬")
        lines.append("æ ¸å¿ƒï¼šçœ‹ç”»é¢å†™è§£è¯´")
        lines.append("=" * 50)
        lines.append("")
        
        for i, scene in enumerate(scenes, 1):
            lines.append(f"ã€åœºæ™¯ {i}ã€‘")
            lines.append(f"æ—¶é—´: {scene.get('start_time', 0):.1f}s - {scene.get('end_time', 0):.1f}s")
            lines.append(f"ç”»é¢: {scene.get('visual_content', 'æœªçŸ¥')}")
            lines.append(f"ç±»å‹: {scene.get('scene_type', 'æœªçŸ¥')}")
            lines.append(f"éŸ³é¢‘: {'åŸå£°' if scene.get('narration_type') == 'original' else 'è§£è¯´'}")
            
            if scene.get('dialogue'):
                lines.append(f"å¯¹ç™½: {scene['dialogue'][:100]}...")
            
            if scene.get('narration'):
                lines.append(f"è§£è¯´: {scene['narration']}")
            
            lines.append("")
        
        return '\n'.join(lines)
    
    def _print_timeline(self, timeline: List[Dict]):
        """æ‰“å°æ—¶é—´çº¿"""
        print("\n" + "="*70)
        print("ğŸ“‹ V4.0 å‰ªè¾‘æ—¶é—´çº¿ï¼ˆè§£è¯´/åŸå£°åˆ†ç¦»ï¼‰")
        print("="*70)
        print(f"{'#':<4} {'æºè§†é¢‘':<20} {'è¾“å‡ºæ—¶é—´':<20} {'éŸ³é¢‘':<10} {'å†…å®¹':<20}")
        print("-"*70)
        
        for item in timeline[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            source = f"{item['source_start']:.1f}s - {item['source_end']:.1f}s"
            output = f"{item['output_start']:.1f}s - {item['output_end']:.1f}s"
            audio = "ğŸ”ŠåŸå£°" if item['audio_mode'] == 'original' else "ğŸ™ï¸è§£è¯´"
            content = item.get('visual_content', '')[:18]
            
            print(f"{item['scene_id']:<4} {source:<20} {output:<20} {audio:<10} {content:<20}")
        
        if len(timeline) > 20:
            print(f"... è¿˜æœ‰ {len(timeline) - 20} ä¸ªåœºæ™¯")
        
        # ç»Ÿè®¡
        original_count = sum(1 for t in timeline if t['audio_mode'] == 'original')
        voiceover_count = len(timeline) - original_count
        print("="*70)
        print(f"æ€»è®¡: {len(timeline)} ä¸ªåœºæ™¯ | åŸå£°: {original_count} | è§£è¯´: {voiceover_count}")
        print("="*70)
    
    def _print_header(self, video, name, style, duration):
        """æ‰“å°å¼€å§‹ä¿¡æ¯"""
        print("\n" + "â˜…"*60)
        print("â˜…  SmartVideoClipper v4.0 - çœ‹ç”»é¢å†™è§£è¯´")
        print("â˜…  ")
        print("â˜…  æ ¸å¿ƒæ”¹è¿›:")
        print("â˜…  1. åŸºäºè§†é¢‘å†…å®¹ç”Ÿæˆè§£è¯´ï¼ˆä¸æ˜¯æ³›æ³›è€Œè°ˆï¼‰")
        print("â˜…  2. è§£è¯´å’ŒåŸå£°äºŒé€‰ä¸€ï¼ˆä¸æ··åˆï¼‰")
        print("â˜…  3. ç”»é¢-è§£è¯´ç²¾ç¡®å¯¹é½")
        print("â˜…  " + "="*52)
        print(f"â˜…  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â˜…  è¾“å…¥è§†é¢‘: {video}")
        print(f"â˜…  ä½œå“åç§°: {name or 'æœªçŸ¥'}")
        print(f"â˜…  è§£è¯´é£æ ¼: {style}")
        print(f"â˜…  ç›®æ ‡æ—¶é•¿: {duration}ç§’")
        print("â˜…"*60 + "\n")
    
    def _print_footer(self, output, elapsed):
        """æ‰“å°å®Œæˆä¿¡æ¯"""
        minutes = elapsed // 60
        seconds = elapsed % 60
        print("\n" + "â˜…"*60)
        print("â˜…  âœ… V4.0 å¤„ç†å®Œæˆï¼")
        print("â˜…  " + "="*52)
        print(f"â˜…  ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â˜…  æ€»è€—æ—¶: {minutes}åˆ†{seconds}ç§’")
        print(f"â˜…  è¾“å‡ºæ–‡ä»¶: {output}")
        print("â˜…"*60)
    
    def _generate_silence(self, output_path: str, duration: float):
        """ç”Ÿæˆé™éŸ³éŸ³é¢‘"""
        import subprocess
        cmd = [
            'ffmpeg', '-y',
            '-f', 'lavfi',
            '-i', f'anullsrc=r=44100:cl=stereo',
            '-t', str(duration),
            '-acodec', 'pcm_s16le',
            output_path
        ]
        subprocess.run(cmd, capture_output=True)


# ä¾¿æ·å‡½æ•°
async def process_video_v4(
    input_video: str,
    movie_name: str = None,
    output_name: str = "è§£è¯´è§†é¢‘",
    style: str = "å¹½é»˜",
    target_duration: int = 600,
    progress_callback: Optional[Callable] = None,
    tmdb_api_key: str = None
) -> dict:
    """V4.0 å¤„ç†å…¥å£"""
    pipeline = VideoPipelineV4()
    return await pipeline.process(
        input_video=input_video,
        movie_name=movie_name,
        output_name=output_name,
        style=style,
        target_duration=target_duration,
        progress_callback=progress_callback,
        tmdb_api_key=tmdb_api_key
    )


# æµ‹è¯•
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        test_video = sys.argv[1]
        movie_name = sys.argv[2] if len(sys.argv) > 2 else None
        
        asyncio.run(process_video_v4(
            input_video=test_video,
            movie_name=movie_name,
            output_name="æµ‹è¯•è¾“å‡º_v4",
            style="å¹½é»˜",
            target_duration=300
        ))
    else:
        print("ç”¨æ³•: python pipeline_v4.py <è§†é¢‘è·¯å¾„> [ä½œå“åç§°]")

