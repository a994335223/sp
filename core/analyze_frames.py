# core/analyze_frames.py - Chinese-CLIPç”»é¢åˆ†æï¼ˆğŸ‡¨ğŸ‡³ å›½å†…ç‰ˆï¼‰
"""
SmartVideoClipper - ç”»é¢åˆ†ææ¨¡å—

åŠŸèƒ½: ä½¿ç”¨Chinese-CLIPåˆ†æè§†é¢‘ç”»é¢ï¼Œè¯†åˆ«åœºæ™¯ç±»å‹
ç”¨é€”: æ‰¾å‡ºé‡è¦é•œå¤´ï¼ˆæ‰“æ–—ã€æµªæ¼«ã€æç¬‘ç­‰ï¼‰

ä¾èµ–: cn-clip, torch, opencv-python, pillow
"""

import torch
from PIL import Image
import cv2
import numpy as np
from typing import List, Dict
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Chinese-CLIP (å›½å†…ç‰ˆï¼Œæ— éœ€VPN)
CLIP_SOURCE = "cn_clip"

try:
    import cn_clip.clip as clip
    from cn_clip.clip import load_from_name
    CLIP_SOURCE = "chinese_clip"
except ImportError:
    try:
        # å¤‡é€‰æ–¹æ¡ˆ
        from cn_clip import clip
        from cn_clip.clip import load_from_name
    except ImportError:
        print("âš ï¸ Chinese-CLIPæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install cn-clip")
        clip = None
        load_from_name = None


class CLIPAnalyzer:
    """ä½¿ç”¨Chinese-CLIPåˆ†æè§†é¢‘ç”»é¢"""
    
    # åœºæ™¯ç±»å‹ï¼ˆä¸­æ–‡ï¼‰
    SCENE_TYPES = [
        "ä¸¤äººå¯¹è¯åœºæ™¯",
        "å¤šäººç¾¤æˆåœºæ™¯",
        "æ‰“æ–—åŠ¨ä½œåœºæ™¯",
        "è¿½é€åœºæ™¯",
        "æµªæ¼«çˆ±æƒ…åœºæ™¯",
        "æ‚²ä¼¤å“­æ³£åœºæ™¯",
        "æç¬‘å¹½é»˜åœºæ™¯",
        "ç´§å¼ æ‚¬ç–‘åœºæ™¯",
        "é£æ™¯ç©ºé•œå¤´",
        "ç‰¹å†™é•œå¤´",
        "æ™®é€šè¿‡æ¸¡é•œå¤´"
    ]
    
    def __init__(self, model_name: str = "ViT-B-16"):
        """
        åˆå§‹åŒ–CLIPåˆ†æå™¨
        
        å‚æ•°:
            model_name: CLIPæ¨¡å‹åç§° (ViT-B-16, ViT-L-14, ViT-H-14)
        """
        if clip is None:
            raise ImportError("Chinese-CLIPæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install cn-clip")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¼ï¸ åŠ è½½Chinese-CLIP (æ¥æº: {CLIP_SOURCE})...")
        print(f"   è®¾å¤‡: {self.device}, æ¨¡å‹: {model_name}")
        
        # åŠ è½½æ¨¡å‹
        self.model, self.preprocess = load_from_name(
            model_name, 
            device=self.device, 
            download_root='./models'
        )
        self.model.eval()
        
        # é¢„è®¡ç®—åœºæ™¯ç±»å‹çš„æ–‡æœ¬ç‰¹å¾
        self._prepare_text_features()
        print("âœ… Chinese-CLIPåŠ è½½å®Œæˆ")
    
    def _prepare_text_features(self):
        """é¢„è®¡ç®—åœºæ™¯ç±»å‹çš„æ–‡æœ¬ç‰¹å¾"""
        text_tokens = clip.tokenize(self.SCENE_TYPES).to(self.device)
        with torch.no_grad():
            self.text_features = self.model.encode_text(text_tokens)
            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)
    
    def analyze_frame(self, frame: np.ndarray) -> Dict:
        """
        åˆ†æå•å¸§ç”»é¢
        
        å‚æ•°:
            frame: OpenCVæ ¼å¼çš„å›¾åƒå¸§ (BGR)
        
        è¿”å›:
            {'top_scene': 'åœºæ™¯ç±»å‹', 'confidence': 0.8, 'all_scores': {...}}
        """
        # è½¬æ¢ä¸ºPILå›¾åƒ
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = (image_features @ self.text_features.T).squeeze()
            probs = similarity.softmax(dim=-1)
        
        # è·å–ç»“æœ
        probs_np = probs.cpu().numpy()
        top_idx = probs_np.argmax()
        
        # è·å–å‰3ä¸ªæœ€å¯èƒ½çš„åœºæ™¯
        top_indices = probs_np.argsort()[-3:][::-1]
        
        return {
            'top_scene': self.SCENE_TYPES[top_idx],
            'confidence': float(probs_np[top_idx]),
            'top3': {
                self.SCENE_TYPES[i]: float(probs_np[i])
                for i in top_indices
            }
        }
    
    def analyze_video_scenes(self, video_path: str, scenes: List[Dict]) -> List[Dict]:
        """
        åˆ†ææ¯ä¸ªé•œå¤´çš„ä¸­é—´å¸§
        
        å‚æ•°:
            video_path: è§†é¢‘è·¯å¾„
            scenes: é•œå¤´åˆ—è¡¨ [{'start': 0, 'end': 5, ...}, ...]
        
        è¿”å›:
            åˆ†æåçš„é•œå¤´åˆ—è¡¨ï¼Œå¢åŠ äº†scene_type, confidence, is_importantå­—æ®µ
        """
        print(f"ğŸ–¼ï¸ å¼€å§‹CLIPç”»é¢åˆ†æ: {len(scenes)}ä¸ªé•œå¤´")
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        analyzed_scenes = []
        for i, scene in enumerate(scenes):
            # å–é•œå¤´ä¸­é—´å¸§
            mid_time = (scene['start'] + scene['end']) / 2
            cap.set(cv2.CAP_PROP_POS_MSEC, mid_time * 1000)
            ret, frame = cap.read()
            
            if ret:
                analysis = self.analyze_frame(frame)
                scene_info = {
                    **scene,
                    'scene_type': analysis['top_scene'],
                    'confidence': analysis['confidence'],
                    'is_important': analysis['confidence'] > 0.3 and 
                                   analysis['top_scene'] not in ['æ™®é€šè¿‡æ¸¡é•œå¤´', 'é£æ™¯ç©ºé•œå¤´']
                }
                analyzed_scenes.append(scene_info)
                
                if (i + 1) % 50 == 0:
                    print(f"  å·²åˆ†æ {i+1}/{len(scenes)} ä¸ªé•œå¤´")
        
        cap.release()
        
        important_count = sum(1 for s in analyzed_scenes if s['is_important'])
        print(f"âœ… åˆ†æå®Œæˆï¼Œå‘ç° {important_count} ä¸ªé‡è¦é•œå¤´")
        
        return analyzed_scenes
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'model'):
            del self.model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•CLIPåˆ†æ
    try:
        analyzer = CLIPAnalyzer()
        
        # å‡è®¾å·²æœ‰é•œå¤´åˆ—è¡¨
        scenes = [{'start': 0, 'end': 5}, {'start': 5, 'end': 10}]
        
        test_video = "test_video.mp4"
        if os.path.exists(test_video):
            analyzed = analyzer.analyze_video_scenes(test_video, scenes)
            
            for scene in analyzed[:5]:
                print(f"é•œå¤´ {scene['start']:.1f}s: {scene['scene_type']} ({scene['confidence']:.2f})")
        else:
            print(f"âš ï¸ æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {test_video}")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")

