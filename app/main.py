# app/main.py - ç®€åŒ–ç‰ˆï¼Œä¸€é”®å¤„ç†2å°æ—¶ç”µå½±ï¼ˆä¸å«è”ç½‘åŠŸèƒ½ï¼‰
"""
SmartVideoClipper - ç®€åŒ–ç‰ˆä¸»ç¨‹åº

åŠŸèƒ½: ä¸€é”®å¤„ç†è§†é¢‘ï¼Œç”ŸæˆæŠ–éŸ³è§£è¯´
ç‰¹ç‚¹: ä¸éœ€è¦è”ç½‘ï¼Œçº¯æœ¬åœ°å¤„ç†

ä½¿ç”¨æ–¹æ³•:
    python app/main.py
"""

import os
import sys
import asyncio
from pathlib import Path

# ğŸ”§ æ·»åŠ é¡¹ç›®è·¯å¾„ï¼ˆç¡®ä¿èƒ½æ‰¾åˆ°æ‰€æœ‰æ¨¡å—ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent  # smart-video-clipper/
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / "core"))  # æ ¸å¿ƒæ¨¡å—ç›®å½•

# ä» utils/ å¯¼å…¥
from utils.gpu_manager import GPUManager

# ä» core/ å¯¼å…¥ï¼ˆå·²æ·»åŠ åˆ°è·¯å¾„ï¼Œç›´æ¥å¯¼å…¥ï¼‰
from scene_detect import detect_scenes
from remove_silence import remove_silence
from transcribe import transcribe_video
from analyze_frames import CLIPAnalyzer
from generate_script import generate_narration_script
from smart_cut import extract_clips, concat_clips, parse_keep_original_markers, select_best_clips
from tts_synthesis import TTSEngine
from compose_video import compose_final_video, convert_to_douyin


async def process_movie(
    input_video: str,
    output_name: str = "æŠ–éŸ³è§£è¯´",
    style: str = "å¹½é»˜åæ§½"
):
    """
    å¤„ç†2å°æ—¶ç”µå½±ï¼Œç”ŸæˆæŠ–éŸ³è§£è¯´è§†é¢‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
    
    å‚æ•°:
        input_video: è¾“å…¥è§†é¢‘è·¯å¾„
        output_name: è¾“å‡ºæ–‡ä»¶å
        style: è§£è¯´é£æ ¼
    """
    work_dir = Path(f"workspace_{output_name}")
    work_dir.mkdir(exist_ok=True)
    
    print("=" * 60)
    print(f"ğŸ¬ å¼€å§‹å¤„ç†: {input_video}")
    print(f"   è§£è¯´é£æ ¼: {style}")
    print("=" * 60)
    
    # ========== Step 1: é•œå¤´åˆ‡åˆ† ==========
    print("\nğŸ“ Step 1/8: é•œå¤´åˆ‡åˆ†...")
    scenes, _ = detect_scenes(input_video, str(work_dir))
    print(f"   æ£€æµ‹åˆ° {len(scenes)} ä¸ªé•œå¤´")
    
    # ========== Step 2: é™éŸ³å‰ªé™¤ï¼ˆå¯é€‰ï¼‰==========
    print("\nğŸ“ Step 2/8: é™éŸ³å‰ªé™¤...")
    # å¯¹äºç”µå½±ï¼Œå¯ä»¥è·³è¿‡è¿™æ­¥ï¼Œç›´æ¥ç”¨åŸç‰‡
    # processed_video = remove_silence(input_video, str(work_dir / "no_silence.mp4"))
    processed_video = input_video
    
    # ========== Step 3: è¯­éŸ³è¯†åˆ« ==========
    print("\nğŸ“ Step 3/8: è¯­éŸ³è¯†åˆ«...")
    segments, transcript = transcribe_video(
        processed_video, 
        str(work_dir / "subtitles.srt")
    )
    print(f"   è¯†åˆ«åˆ° {len(segments)} æ®µå¯¹ç™½")
    
    # ========== Step 4: CLIPç”»é¢åˆ†æ ==========
    print("\nğŸ“ Step 4/8: CLIPç”»é¢åˆ†æ...")
    analyzer = CLIPAnalyzer()
    analyzed_scenes = analyzer.analyze_video_scenes(processed_video, scenes)
    important_scenes = [s for s in analyzed_scenes if s.get('is_important')]
    print(f"   å‘ç° {len(important_scenes)} ä¸ªé‡è¦é•œå¤´")
    
    # ğŸ”§ ç»Ÿä¸€ä½¿ç”¨GPUManageræ¸…ç†æ˜¾å­˜
    del analyzer
    GPUManager.clear()
    
    # ========== Step 5: AIç”Ÿæˆæ–‡æ¡ˆ ==========
    print("\nğŸ“ Step 5/8: AIç”Ÿæˆè§£è¯´æ–‡æ¡ˆ...")
    script = generate_narration_script(transcript, analyzed_scenes, style)
    
    # ä¿å­˜æ–‡æ¡ˆ
    script_file = work_dir / "è§£è¯´æ–‡æ¡ˆ.txt"
    script_file.write_text(script, encoding='utf-8')
    print(f"   æ–‡æ¡ˆå·²ä¿å­˜: {script_file}")
    
    # ========== Step 6: æ™ºèƒ½å‰ªè¾‘ ==========
    print("\nğŸ“ Step 6/8: æ™ºèƒ½å‰ªè¾‘...")
    
    # ğŸ”§ è¾¹ç•Œæƒ…å†µï¼šå¦‚æœæ²¡æœ‰é‡è¦é•œå¤´ï¼Œä½¿ç”¨æ‰€æœ‰é•œå¤´
    if len(important_scenes) == 0:
        print("   âš ï¸ æœªæ£€æµ‹åˆ°é‡è¦é•œå¤´ï¼Œä½¿ç”¨æ‰€æœ‰é•œå¤´")
        important_scenes = analyzed_scenes
    
    # é€‰å–é‡è¦é•œå¤´ï¼ˆæ§åˆ¶æ€»æ—¶é•¿3-5åˆ†é’Ÿï¼‰
    selected_clips = select_best_clips(important_scenes, target_duration=240)
    
    # ğŸ”§ è¾¹ç•Œæƒ…å†µï¼šå¦‚æœé€‰ä¸­ç‰‡æ®µä¸ºç©ºï¼Œè‡³å°‘é€‰å–å‰å‡ ä¸ª
    if len(selected_clips) == 0:
        print("   âš ï¸ ç‰‡æ®µé€‰å–ä¸ºç©ºï¼Œä½¿ç”¨å‰5ä¸ªé•œå¤´")
        selected_clips = [{'start': s['start'], 'end': s['end']} for s in analyzed_scenes[:5]]
    
    extract_clips(processed_video, selected_clips, str(work_dir / "clips"))
    
    clip_files = sorted((work_dir / "clips").glob("*.mp4"))
    concat_clips([str(f) for f in clip_files], str(work_dir / "å‰ªè¾‘å.mp4"))
    
    # ========== Step 7: è¯­éŸ³åˆæˆ ==========
    print("\nğŸ“ Step 7/8: è¯­éŸ³åˆæˆ...")
    tts = TTSEngine("edge")  # ä½¿ç”¨Edge-TTSï¼ˆæ›´ç¨³å®šï¼‰
    await tts.synthesize(script, str(work_dir / "narration.wav"))
    del tts
    GPUManager.clear()  # ğŸ”§ TTSåæ¸…ç†æ˜¾å­˜
    
    # ========== Step 8: è§†é¢‘åˆæˆ ==========
    print("\nğŸ“ Step 8/8: è§†é¢‘åˆæˆ...")
    keep_original = parse_keep_original_markers(script)
    
    compose_final_video(
        str(work_dir / "å‰ªè¾‘å.mp4"),
        str(work_dir / "narration.wav"),
        str(work_dir / "æˆå“_æ¨ªå±.mp4"),
        keep_original_segments=keep_original,
        subtitle_path=str(work_dir / "subtitles.srt"),  # ğŸ”§ æ·»åŠ å­—å¹•
        mode="mix"
    )
    
    # è½¬æ¢æŠ–éŸ³æ ¼å¼
    final_output = work_dir / f"{output_name}.mp4"
    convert_to_douyin(
        str(work_dir / "æˆå“_æ¨ªå±.mp4"),
        str(final_output)
    )
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {final_output}")
    print(f"ğŸ“ è§£è¯´æ–‡æ¡ˆ: {script_file}")
    print("=" * 60)
    
    return str(final_output)


# è¿è¡Œ
if __name__ == "__main__":
    # é»˜è®¤æµ‹è¯•
    test_video = "test_video.mp4"
    
    if len(sys.argv) > 1:
        test_video = sys.argv[1]
    
    if os.path.exists(test_video):
        asyncio.run(process_movie(
            test_video,
            output_name="è§£è¯´è§†é¢‘",
            style="å¹½é»˜åæ§½"
        ))
    else:
        print(f"âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_video}")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python app/main.py è§†é¢‘æ–‡ä»¶.mp4")

